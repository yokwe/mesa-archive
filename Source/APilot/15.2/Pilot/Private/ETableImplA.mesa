-- Copyright (C) 1986, 1987, 1988 by Xerox Corporation. All rights reserved. -- ETableImplA.mesa             29-Jul-88 18:58:59  RSVDIRECTORY  BackingStore USING [Run],  DiskBackingStore USING [ChannelHandle, Data, GetDiskChannel,    PDiskDataFromPBSData, Transfer, UnpackFilePageNumber],  DiskChannel USING [defaultTries, goodCompletion, DoIO, Handle, IORequest],  Environment USING [Base, PageCount, PageNumber, wordsPerPage],  ETable USING [BucketHandle, BucketHeader, Error, ErrorType, ETable,    ETableHandle, ETableHeader, FileHeaderHandle, fileSystemSeal,    fileSystemVersion, FreeSpaceInOverflow, LVBucketInfo, LVBucketInfoData,    PageGroup, PageGroupHandle, PageGroupSeq, SingleHeaderHandle,    WhichETableFile],  ETableInternal USING [    fileHeaderSize, GetSize, normalBucketSize, nullETableFilePage,    overflowBufferSize, WhereIsFilePageOnVolume],  File USING [ID, nullID, PageCount, PageNumber],  FileBasicsPerf USING [],  FileBasicsPrograms USING [],  FileLock USING [lockingEnabled, nullLockHandle],  Inline USING [LongCOPY, LowHalf],  KernelFile USING [Descriptor, PageGroup],  KernelSpaceUsage USING [volumeFileMap],  LogicalVolumeFormat USING [Handle, rootPageSize],  PilotFileTypes USING [PilotFileType],  PilotFileTypesExtraExtras USING [tETable],  ResidentHeap USING [first64K, FreeNode, MakeNode],  RuntimeInternal USING [Bug],  Space USING [    Allocate, Deallocate, InsufficientSpace, Interval, LongPointerFromPage,    PageFromLongPointer],  VM USING [    BackingStoreRuns, CopyIn, CopyOut, ForceOut, Interval,    MakeReadOnly, MakeResident, MakeSwappable, MakeWritable, Map, nullInterval,    ScratchMap, SupplyBackingStore, Unmap],  VolAllocMap USING [AllocPageGroup, SetPageGroup],  VolTable USING [    Context, FindSV, GetLVHandle, GetLVID, GetLVStatus, GetETableContext,    LVToken, SetETableContext, SVDesc],  Volume USING [ID, InsufficientSpace, PageCount, PageNumber],  Zone USING [BlockSize, Status];   -- RESTRICTIONS & NOTES  -- ETFETables cannot be greater than 256 pages in length. See   -- ETFileETableCopyOut.  -- Insure page group break after BUCKETS?  (no.)  -- As defined DON'T have the ability to make entries to only one preImageLog.  --  GrowOverflow WANTS this, so I implemented individual NOT both.  -- Logger only understands logging pages that are described by an ETFETable,  --  should it take a generic ETable parameter rather than lvBucketInfo?  -- NO interval that you want to MAP here should span greater than 10 runs!!  -- VOLUMEPAGE: make sure it's pv relative (use FindSV) not just lv relative.ETableImplA: MONITOR  IMPORTS DiskBackingStore, DiskChannel, ETable, ETableInternal, FileLock,   Inline, ResidentHeap, RuntimeInternal, Space, VM, VolAllocMap,   VolTable, Volume   EXPORTS ETable, ETableInternal, FileBasicsPerf, FileBasicsPrograms =  BEGIN  OPEN ETI: ETableInternal, LVF: LogicalVolumeFormat;    BugType: TYPE = {    allocationError, fileSystemInconsistency, fileExcessivelyFragmented,    freeError, impossibleCase, outOfVM, lvInfoStillInUse, noSuchRun, noSuchSV,    notImplemented};     Error: PUBLIC ERROR [error: ETable.ErrorType] = CODE;      -- TYPEs defining the LVBucketInfo list:    ListBase: TYPE = Environment.Base;  BInfoRelPtr: TYPE = ListBase RELATIVE POINTER TO ETable.LVBucketInfoData;  list: ListBase = ResidentHeap.first64K;  nil: Environment.Base RELATIVE POINTER = LOOPHOLE[0];  nilLVT: VolTable.LVToken = LOOPHOLE[0];  -- seperate type for compiler  ETableToken: PUBLIC TYPE = BInfoRelPtr;    Window: TYPE = RECORD [    etable: ETable.ETableHandle,    base: Volume.PageNumber,    count: Volume.PageCount];  AllocationSource: TYPE = {frame, createdSpace};  GroupSequence: TYPE = ETable.PageGroupSeq;  GroupSequenceHandle: TYPE = RECORD [    pageGroups: ETable.PageGroupHandle,    storage: SELECT allocated: AllocationSource FROM      frame => [], createdSpace => [interval: Space.Interval] ENDCASE];  ImageLog: TYPE =  ARRAY [0..0) OF WORD;  ImageLogHandle: TYPE = LONG POINTER TO ImageLog;  Operation: TYPE = {read, write};  -- used for ETable header IO  -- global Variables:  lvInfoUnlocked: CONDITION;  -- NOTIFY any time a buffer is unlocked.    headOfList: BInfoRelPtr; -- head of list of lvBucketInfo recods  firstOpened: BInfoRelPtr;  -- points to first lvBucketInfo created.  headerSize: CARDINAL = ETI.fileHeaderSize;  -- size of ETable.FileHeader.  maxRunSize: CARDINAL ¬ 10;  -- for intervals of ETableFiles of interest.  -- Perf variables    totalAcquireBuckets, acquireBucketWaits: PUBLIC --FileBasicsPerf--    LONG CARDINAL ¬ 0;  -- above variables apply across all lvs. They might be interesting because   -- VolTable allows multiple read locks, but this impl doesn't.  --How to get a LONG POINTER to lvBucketInfo from ResidentHeap relative pointer.  --infoPtr: LVBucketInfo = @list[biRelPtr:BInfoRelPtr];  Bug: PROCEDURE [bugType: BugType] = {RuntimeInternal.Bug[bugType]};  BugV: PROCEDURE [bugType: BugType] RETURNS [UNSPECIFIED] = {    Bug[bugType]; RETURN[0]};  InitETable: PUBLIC PROC = {};  -- it's tradition.  AcquireLVBucketInfo: PUBLIC --ETableInternal-- ENTRY PROCEDURE    [token: VolTable.LVToken]    RETURNS [ETable.LVBucketInfo] =    -- This call locks the lvBucketInfo record that belongs to this LV.    -- The lvtoken uniquely identifies this record and must be used by ETable    -- procedures that manipulate lvBucketInfo. ETable.Open must have been called    -- at some point before this procedure.    -- You can't lock lvBucketInfo without an LVToken because lvBucketInfo points    -- to the token when it's in the locked state.    BEGIN    infoRelPtr: BInfoRelPtr ¬ FindBucketInfo[token];    IF infoRelPtr = nil THEN RETURN WITH ERROR ETable.Error[volumeNotOpen];    totalAcquireBuckets ¬ totalAcquireBuckets.SUCC;    DO      IF list[infoRelPtr].token ~= nilLVT THEN {        acquireBucketWaits ¬ acquireBucketWaits.SUCC;	WAIT lvInfoUnlocked; LOOP};      list[infoRelPtr].token ¬ token;    EXIT;    ENDLOOP;    RETURN[@list[infoRelPtr]];  -- returns a long pointer.    END;          AddBuckets: PUBLIC --ETable-- PROC [lvBucketInfo: ETable.LVBucketInfo,    count: CARDINAL] =    -- does NOT update HeaderPage.numberOfBuckets, client must do that.    -- ASSUMES: the following fields in primary also apply to copy ETF:    -- firstBucketPage, numberOfBuckets, lvHandle, token, maxETableSize,     -- preImageSize    BEGIN    freePages, pagesForET: Volume.PageCount;    index, howManyGroups: CARDINAL;    volumePage, startPage: Volume.PageNumber;    changingET: ETable.ETableHandle;    containingPageGroup: ETable.PageGroup;    defaultGroupCount: CARDINAL = 10;    defaultPageGroup: ARRAY [0..defaultGroupCount) OF ETable.PageGroup;    groupSequence: GroupSequenceHandle ¬      [DESCRIPTOR[defaultPageGroup], frame[]];    IF (freePages ¬ lvBucketInfo.lvHandle.freePageCount) <= (2*count)    THEN ERROR Volume.InsufficientSpace[freePages, lvBucketInfo.lvHandle.vID];    lvBucketInfo.fileHandle.numberOfBucketsToAdd ¬ count;    DoHeaderIO[lvBucketInfo, primary, write,,];    DoHeaderIO[lvBucketInfo, copy, write,,];    FOR which: ETable.WhichETableFile IN [primary..both) DO BEGIN      SELECT which FROM	primary => {	 changingET ¬ lvBucketInfo.primaryETableHandle;	  -- Lets log both ETableFile ETables now.  Order is important.          -- Logging machinery is also going to look at lvBucketInfos ETables           -- to derive Volume.PageNumbers.	 Logger[preImage: changingET,	   count: lvBucketInfo.fileHandle.maxETableSize,	   lvInfo: lvBucketInfo];	 Logger[preImage: lvBucketInfo.copyETableHandle,	   count: lvBucketInfo.fileHandle.maxETableSize,	   lvInfo: lvBucketInfo];	 lvBucketInfo.fileHandle.itemLogged ¬ eTFET;	 lvBucketInfo.fileHandle.firstFilePageLogged ¬ headerSize + lvBucketInfo.fileHandle.preImageSize;	 DoHeaderIO[lvBucketInfo, primary, write,,];	 DoHeaderIO[lvBucketInfo, copy, write,,];};	copy =>	 changingET ¬ lvBucketInfo.copyETableHandle;	both => Bug[impossibleCase];        ENDCASE => Bug[impossibleCase];      -- we want to keep all ETableFile pages close together on disk, no?      startPage ¬ ETI.WhereIsFilePageOnVolume[        lvBucketInfo.firstBucketPage, changingET].volumePage;      GetPages[startPage, count, lvBucketInfo.lvHandle, lvBucketInfo.token,        @groupSequence];      howManyGroups ¬ changingET.header.howManyGroups;      pagesForET ¬        (ETableSizeInWords[howManyGroups + LENGTH[groupSequence.pageGroups]] /	Environment.wordsPerPage) + 1;      IF pagesForET > lvBucketInfo.fileHandle.maxETableSize THEN      ERROR ETable.Error[tooFragmented];      -- adjust page groups in etable before inserting new groups      [volumePage, containingPageGroup, index] ¬        ETI.WhereIsFilePageOnVolume[	  lvBucketInfo.firstOverflowPage - 1, changingET];      howManyGroups ¬ changingET.header.howManyGroups;      BEGIN  -- of insert new page groups into ETable.        -- I decided that handling case of last bucket being in middle of a	-- pageGroup is easier than insuring that last bucket ends a pageGroup at	-- CreateFile. But...        howManyNewGroups: CARDINAL ¬ 0;	tempBuf: ETable.BucketHandle;	-- newCount is count for pageGroups[index] when last bucket occurs in	-- the middle of this group.        newCount: CARDINAL ¬	  containingPageGroup.count -	  CARDINAL[ (containingPageGroup.volumePage + containingPageGroup.count -	    1) - volumePage];	IF newCount # 0 THEN	  howManyNewGroups ¬ LENGTH[groupSequence.pageGroups] + 1	ELSE howManyNewGroups ¬ LENGTH[groupSequence.pageGroups];	VM.MakeWritable[	  [Space.PageFromLongPointer[changingET],	  lvBucketInfo.fileHandle.maxETableSize]];	IF howManyGroups # (index + 1) THEN	  -- make space in ETable for new page groups	  Inline.LongCOPY[	    from: @changingET.pageGroups[index + 1],	    nwords: (howManyGroups - index - 1) * SIZE[ETable.PageGroup],	    to: @changingET.pageGroups[howManyGroups + howManyNewGroups]];	changingET.pageGroups[index] ¬ [containingPageGroup.volumePage, newCount];	IF newCount # 0 THEN BEGIN	  -- this group starts with first overflow page.	  changingET.pageGroups[index + 1 + howManyGroups].count ¬	    containingPageGroup.count - newCount;	  changingET.pageGroups[index + 1 + howManyGroups].volumePage ¬	    containingPageGroup.volumePage + newCount;	  END;	-- now squeeze in new bucket pages between old last and first overflow.	Inline.LongCOPY[	  from: @groupSequence.pageGroups[0],	  nwords: LENGTH[groupSequence.pageGroups] * SIZE[ETable.PageGroup],	  to: @changingET.pageGroups[index + 1]];	-- The above does the following:	-- FOR i: CARDINAL IN [0..LENGTH[groupSequence.pageGroups]) DO	--   changingET.pageGroups[index + 1 + i] ¬ groupSequence.pageGroups[i];	--   ENDLOOP;	-- oh piss, we still have to write bucket headers...	tempBuf ¬ LOOPHOLE[Space.Allocate[headerSize !	  Space.InsufficientSpace => Bug[outOfVM]].pointer];	VM.ScratchMap[          [Space.PageFromLongPointer[tempBuf], ETI.normalBucketSize]];	-- lvBucketInfo reflects old number of buckets still.	-- WriteBucketHeaders Deallocates tempBuf for us.        WriteBucketHeaders[	  bucketBuf: tempBuf,	  currentPage: lvBucketInfo.firstOverflowPage,	  lastPage: lvBucketInfo.firstOverflowPage + count - 1,	  lvHandle: lvBucketInfo.lvHandle,	  etable: changingET,	  writeOverflow: FALSE,	  firstOverflow: lvBucketInfo.firstOverflowPage --TRASH--];	changingET.header.howManyGroups ¬	  (howManyGroups + LENGTH[groupSequence.pageGroups]);      END;  -- of adding new pageGroups      VM.ForceOut[	[Space.PageFromLongPointer[changingET],	 lvBucketInfo.fileHandle.maxETableSize],	 wait];      VM.MakeReadOnly[        [Space.PageFromLongPointer[changingET],        lvBucketInfo.fileHandle.maxETableSize]];      END;    ENDLOOP;    CopyPrimariesToCopies[lvBucketInfo.token, lvBucketInfo.lvHandle,,,];    lvBucketInfo.firstOverflowPage ¬ lvBucketInfo.firstOverflowPage + count;    lvBucketInfo.currentOverflowPage ¬ lvBucketInfo.currentOverflowPage + count;    FinishWithSequence[groupSequence];    END;  -- of AddBuckets --  --How to get a relative pointer from a LONG POINTER to LVBucketInfoData.  BInfoRelFromPtr: PROCEDURE [bi: ETable.LVBucketInfo]    RETURNS [biRelPtr: BInfoRelPtr] = INLINE {RETURN[Inline.LowHalf[bi - list]]};    ClearLog: PROC [lvInfo: ETable.LVBucketInfo] =    BEGIN    lvInfo.fileHandle.numberOfPagesLogged ¬ 0;    lvInfo.fileHandle.itemLogged ¬ nothingLogged;    DoHeaderIO[lvInfo, primary, write,,];    DoHeaderIO[lvInfo, copy, write,,];    END;    Close: PUBLIC --ETable-- ENTRY PROCEDURE [token: VolTable.LVToken] =    -- Called by Volume.Close.  We can now clean up runtime ETable structures    -- for this volume.  If volume never ETable.Open(ed) that's ok.    BEGIN ENABLE UNWIND => NULL;    infoRelPtr, prevInfoRelPtr: BInfoRelPtr;    infoPtr: ETable.LVBucketInfo;    clientVID: Volume.ID ¬ VolTable.GetLVID[token];    IF headOfList = nil THEN RETURN;    infoRelPtr ¬ prevInfoRelPtr ¬ headOfList;    -- Remember list only contains LVBucketInfos for currently open lvs.    -- Don't use FindBucketInfo here 'cause we need prevInfoRelPtr.    IF list[infoRelPtr].lvHandle.vID ~= clientVID THEN {      infoRelPtr ¬ list[infoRelPtr].nextLVBucketInfo;      IF infoRelPtr = nil THEN RETURN;      UNTIL list[infoRelPtr].lvHandle.vID = clientVID OR infoRelPtr = nil DO        prevInfoRelPtr ¬ infoRelPtr;	infoRelPtr ¬ list[infoRelPtr].nextLVBucketInfo;      ENDLOOP};    IF infoRelPtr = nil THEN RETURN;    -- infoRelPtr points to the lvBucketInfo we want to free.    infoPtr ¬ @list[infoRelPtr];    IF infoPtr.token ~= nilLVT THEN Bug[lvInfoStillInUse];    VolTable.SetETableContext[tok: token, context: LOOPHOLE[nil]];    UnmapHeader[infoPtr];    UnmapLVRootPage[infoPtr];    UnmapETFETables[infoPtr];    UnmapOverflowWindow[infoPtr];    -- free lvBucketInfo itself.    list[prevInfoRelPtr].nextLVBucketInfo ¬ list[infoRelPtr].nextLVBucketInfo;    IF infoRelPtr = headOfList THEN      headOfList ¬ list[infoRelPtr].nextLVBucketInfo;    FreeInfo[infoRelPtr];    END;  -- Close --          ConvertTypeToID: PUBLIC --ETableInternal-- PROCEDURE [   type: PilotFileTypes.PilotFileType] RETURNS [File.ID] =   {RETURN[LOOPHOLE[LONG[type], File.ID]]};        CopyPrimariesToCopies: PROC [tok: VolTable.LVToken, lv: LVF.Handle, canTrustLV: BOOLEAN ¬ TRUE, vpp, vpc: Volume.PageNumber] =    -- need to get the ETables that describe each ETableFile copied into the    -- the other ETableFile.    -- etfOne LVBucketInfoData is mapped to ETF1 primary and ETF2 primary.    -- oneCopy is mapped to ETF1's pages for copy of ETF2 ETable.    -- twoCopy is mapped to ETF2's pages for copy of ETF1 ETable.    -- To CopyPrimariesToCopies just copy the stuff in etfOne to oneCopy and     -- twoCopy.    BEGIN    etfOne: ETable.LVBucketInfo;    oneCopy, twoCopy: ETable.ETableHandle;    etStartPage: File.PageNumber;    maxETSize: File.PageCount;    etfOne ¬ @list[MakeNewInfo[SIZE[ETable.LVBucketInfoData]]];    etfOne.token ¬ tok;    MapLVRootPage[etfOne];  -- we use VolTable tok for this.    DoHeaderIO[etfOne, primary, read, canTrustLV, vpp];    MapETFETable[etfOne, primary];    MapETFETable[etfOne, copy];    maxETSize ¬ etfOne.fileHandle.maxETableSize;    oneCopy ¬ LOOPHOLE[Space.Allocate[maxETSize      ! Space.InsufficientSpace => Bug[outOfVM]].pointer];    etStartPage ¬      headerSize + etfOne.fileHandle.preImageSize + maxETSize;    MapRuns[lv, oneCopy,      [etable: etfOne.primaryETableHandle, base: etStartPage, count: maxETSize]];    twoCopy ¬ LOOPHOLE[Space.Allocate[maxETSize      ! Space.InsufficientSpace => Bug[outOfVM]].pointer];    etStartPage ¬      headerSize + etfOne.fileHandle.preImageSize;    MapRuns[lv, twoCopy,      [etfOne.copyETableHandle, etStartPage, maxETSize]];    ETFileETableCopyOut[      destination: twoCopy,      source: etfOne.primaryETableHandle,      sourcePageCount: etfOne.fileHandle.maxETableSize];    ETFileETableCopyOut[      destination: oneCopy,      source: etfOne.copyETableHandle,      sourcePageCount: etfOne.fileHandle.maxETableSize];    VM.Unmap[Space.PageFromLongPointer[oneCopy]];    VM.Unmap[Space.PageFromLongPointer[twoCopy]];    Space.Deallocate[[oneCopy, maxETSize]];    Space.Deallocate[[twoCopy, maxETSize]];    UnmapHeader[etfOne];    FreeInfo[BInfoRelFromPtr[etfOne]];    END;  -- of CopyPrimariesToCopies --  CreateFile: PUBLIC --ETable-- PROCEDURE [    lvHandle: LVF.Handle, tok: VolTable.LVToken,    primaryStartPageHint: Volume.PageNumber,    copyStartPageHint: Volume.PageNumber, eTableSize: CARDINAL,    preImageSize: CARDINAL, numberOfBuckets: CARDINAL,    overflowSize: LONG CARDINAL] =    BEGIN    -- writes two EtableFiles to disk and updates lvRoot page to point at them.    -- May raise InsuffucientSpace but will not have released pages it already    -- allocated to ETableFiles. Caller must clear VAM and start again.    -- NOBODY should be allocating pages when this is running anyway!!!    -- FORMAT: headerPage ! preImageLog ! ETFETable ! ETFETable ! Buckets ! Overflow    desiredPages: File.PageCount;          freePages: Volume.PageCount;    memPage: Environment.PageNumber;        flavorOfETable: TYPE = ETable.WhichETableFile[primary.. copy];    startPage, startPageHint: ARRAY flavorOfETable OF Volume.PageNumber;    -- buffer that ETFHeader is built in and then mapped to correct page.    headerPageBuffer: ETable.FileHeaderHandle ¬ NIL;    defaultRunSize: CARDINAL = headerSize;    runs: ARRAY [0..defaultRunSize) OF BackingStore.Run;    diskData: LONG POINTER TO DiskBackingStore.Data ¬      DiskBackingStore.PDiskDataFromPBSData[@runs[0].data];    channel: DiskBackingStore.ChannelHandle;    sv: VolTable.SVDesc;    found: BOOLEAN;            CreateInternal: PROCEDURE [      lvHandle: LVF.Handle, tok: VolTable.LVToken, flavor: flavorOfETable,      initialSize: File.PageCount, startPage: Volume.PageNumber]      RETURNS[firstPage: Volume.PageNumber] =      -- Caller must have VolTable.LVToken write lock.      BEGIN      offsetOfET1, offsetOfET2: Volume.PageCount;  -- used to set firstETableFilePage in header.      pagesForET: Volume.PageCount ¬ 0;      h: ETable.SingleHeaderHandle;       myETable: ETable.ETableHandle;      preImage: ImageLogHandle;      defaultRunSize, defaultGroupCount: CARDINAL = 10;        --worst case would be run per ETable page.	-- should defaultRunSize be MAX[eTableSize, preImageSize]?      defaultPageGroups: ARRAY [0..defaultGroupCount) OF ETable.PageGroup;      groupSequence: GroupSequenceHandle ¬        [DESCRIPTOR[defaultPageGroups], frame[]];      defaultRuns: ARRAY [0..defaultRunSize) OF BackingStore.Run;      runs: VM.BackingStoreRuns ¬ DESCRIPTOR[defaultRuns];      fileD: KernelFile.Descriptor ¬ [	fileID: ConvertTypeToID[PilotFileTypesExtraExtras.tETable],        volumeID: lvHandle.vID, temporary: FALSE,        size: 0, type: PilotFileTypesExtraExtras.tETable];      --  MAIN of CreateInternal --      GetPages[startPage, initialSize, lvHandle, tok, @groupSequence];      -- check size of ETFETable      pagesForET ¬         (ETableSizeInWords[LENGTH[groupSequence.pageGroups]] /        Environment.wordsPerPage) + 1;      IF pagesForET > eTableSize THEN ERROR ETable.Error[tooFragmented];      myETable ¬ LOOPHOLE[Space.Allocate[pagesForET !        Space.InsufficientSpace => Bug[outOfVM]].pointer];      VM.ScratchMap[	[Space.PageFromLongPointer[myETable], pagesForET],	KernelSpaceUsage.volumeFileMap];          -- create ETFETable      myETable.header.howManyGroups ¬ LENGTH[groupSequence.pageGroups];      myETable.header.fileID ¬	ConvertTypeToID[PilotFileTypesExtraExtras.tETable];      myETable.header.temporary ¬ FALSE;      myETable.header.bootable ¬ FALSE;      myETable.header.type ¬ PilotFileTypesExtraExtras.tETable;      Inline.LongCOPY[        from: @groupSequence.pageGroups[0],	nwords: LENGTH[groupSequence.pageGroups] * SIZE[ETable.PageGroup],	to: @myETable.pageGroups[0]];      -- The above does the following:      -- FOR i:CARDINAL IN [0..LENGTH[groupSequence.pageGroups]) DO      --   myETable.pageGroups[i] ¬ groupSequence.pageGroups[i]; ENDLOOP;      FinishWithSequence[groupSequence];          SELECT flavor FROM        primary => {	  offsetOfET1 ¬ 0;	  offsetOfET2 ¬ eTableSize;	  h ¬ @headerPageBuffer.primaryETFHeader};	copy => {	  offsetOfET1 ¬ eTableSize;	  offsetOfET2 ¬ 0;	  h ¬ @headerPageBuffer.copyETFHeader};            ENDCASE => Bug[impossibleCase];      -- build runs      runs ¬ GetBSRuns[	lvHandle: lvHandle,	runs: runs,	window:	  [etable: myETable,	   base: headerSize + preImageSize + offsetOfET1,	   count: eTableSize]	  ];      VM.SupplyBackingStore[	page: Space.PageFromLongPointer[myETable],	transferProc: DiskBackingStore.Transfer, backingStoreRuns: runs,	swapUnits: [uniform[1]]];          -- get these pages to disk      VM.MakeSwappable[	[Space.PageFromLongPointer[myETable], eTableSize]];      VM.ForceOut[	interval: [Space.PageFromLongPointer[myETable], eTableSize],	returnWait: wait];      -- what about rest of maxETableSize area?                  -- I know ETFHeader page is file page 0.      h.firstPreImagePage ¬ ETI.WhereIsFilePageOnVolume[page:	headerSize, eTable: myETable].volumePage;      --      -- The physical ordering of the next two fields on disk vary dependent upon      -- whether this is the primary or copy ETableFile we are creating.      --      h.myFirstETablePage ¬ ETI.WhereIsFilePageOnVolume[	headerSize + preImageSize + offsetOfET1, myETable].volumePage;      h.otherFirstETablePage ¬ ETI.WhereIsFilePageOnVolume[	headerSize + preImageSize + offsetOfET2, myETable].volumePage;      h.firstOverflowPage ¬ ETI.WhereIsFilePageOnVolume[	headerSize + preImageSize + (2*eTableSize) + numberOfBuckets,	myETable].volumePage;      preImage ¬ LOOPHOLE[Space.Allocate[1 	! Space.InsufficientSpace => Bug[outOfVM]].pointer];      VM.ScratchMap[        [Space.PageFromLongPointer[preImage], ETI.normalBucketSize]];      -- write bucket headers      -- WriteBucketHeaders will Space.Deallocate preImage buffer.      FOR i:CARDINAL IN [0..Environment.wordsPerPage) DO preImage[i] ¬ 0; ENDLOOP;      WriteBucketHeaders[	bucketBuf: LOOPHOLE[preImage, ETable.BucketHandle],	currentPage: headerSize + preImageSize + (2*eTableSize),	lastPage: headerSize + preImageSize + (2*eTableSize) +	  numberOfBuckets - 1,	lvHandle: lvHandle,	etable: myETable,	writeOverflow: TRUE,	firstOverflow: h.firstOverflowPage];            firstPage ¬ ETI.WhereIsFilePageOnVolume[0, myETable].volumePage;      VM.Unmap[Space.PageFromLongPointer[myETable]];      Space.Deallocate[[myETable, 1]];      RETURN[firstPage];    END;  --CreateInternal--    -- BEGIN main CreateFile --    runs[0].count ¬ headerSize;        -- roundup overflowSize up if needed.            startPageHint[primary] ¬ primaryStartPageHint;    startPageHint[copy] ¬ copyStartPageHint;    IF overflowSize = 0 THEN overflowSize ¬ 4;    IF (overflowSize MOD 4) # 0 THEN      overflowSize ¬ overflowSize + (4 - (overflowSize MOD 4));    desiredPages ¬      headerSize + (2*eTableSize) + preImageSize +      numberOfBuckets + overflowSize;    IF (freePages ¬ lvHandle.freePageCount) < (2*desiredPages)      THEN ERROR Volume.InsufficientSpace[freePages, lvHandle.vID];    -- map Header page    headerPageBuffer ¬ Space.Allocate[headerSize !      Space.InsufficientSpace => Bug[outOfVM]].pointer;    VM.ScratchMap[interval:      [Space.PageFromLongPointer[headerPageBuffer], headerSize]];    memPage ¬ Space.PageFromLongPointer[headerPageBuffer];    headerPageBuffer.seal ¬ ETable.fileSystemSeal;    headerPageBuffer.version ¬ ETable.fileSystemVersion;    headerPageBuffer.lv ¬ lvHandle.vID;    headerPageBuffer.numberOfBuckets ¬ numberOfBuckets;    headerPageBuffer.numberOfBucketsToAdd ¬ 0;    headerPageBuffer.maxETableSize ¬ eTableSize;    headerPageBuffer.overflowSize ¬ overflowSize;    headerPageBuffer.preImageSize ¬ preImageSize;    headerPageBuffer.numberOfPagesLogged ¬ 0;    headerPageBuffer.itemLogged ¬ nothingLogged;    headerPageBuffer.firstFilePageLogged ¬ 0;    FOR etableIndex: ETable.WhichETableFile IN flavorOfETable DO      startPage[etableIndex] ¬	CreateInternal[	  lvHandle: lvHandle, tok: tok, flavor: etableIndex,	  initialSize: desiredPages, startPage: startPageHint[etableIndex]];    ENDLOOP;    [found, channel] ¬ VolTable.FindSV[lvHandle.vID,     startPage[primary], @sv];    IF ~found THEN Bug[noSuchSV];    --diskData.file ¬ ConvertTypeToID[PilotFileTypesExtraExtras.tETable];    WITH d: diskData SELECT FileLock.lockingEnabled FROM      TRUE => d.lock ¬ FileLock.nullLockHandle;      FALSE => d.file ¬ ConvertTypeToID[PilotFileTypesExtraExtras.tETable];      ENDCASE;    diskData.type ¬ PilotFileTypesExtraExtras.tETable;    diskData.channelHandle ¬ channel;    diskData.volumePage ¬ sv.pvPageOfSV + startPage[primary];    [diskData.filePageLow, diskData.filePageHigh] ¬      DiskBackingStore.UnpackFilePageNumber[startPage[primary]];    diskData.fileAttributes ¬ [temporary: FALSE, readOnly: FALSE];    VM.CopyOut[[memPage, headerSize],      DiskBackingStore.Transfer, runs[0], wait];    diskData.volumePage ¬ sv.pvPageOfSV + startPage[copy] ;    [diskData.filePageLow, diskData.filePageHigh] ¬      DiskBackingStore.UnpackFilePageNumber[startPage[copy]];    VM.CopyOut[[memPage, headerSize],      DiskBackingStore.Transfer, runs[0], wait];    CopyPrimariesToCopies[tok, lvHandle, FALSE, startPage[primary], startPage[copy]];    lvHandle.primaryETableStartPage ¬ startPage[primary];    lvHandle.copyETableStartPage ¬ startPage[copy];    VM.ForceOut[[Space.PageFromLongPointer[lvHandle], LVF.rootPageSize], wait];    VM.Unmap[Space.PageFromLongPointer[headerPageBuffer]];    Space.Deallocate[[headerPageBuffer, ETI.fileHeaderSize]];    END;  -- of CreateFile--  DeleteFile: PUBLIC PROCEDURE [    lvBucketInfo: ETable.LVBucketInfo, whichFile: ETable.WhichETableFile,    eTable: ETable.ETableHandle] = {Bug[notImplemented]};    DoHeaderIO: PROC [    lvInfo: ETable.LVBucketInfo, which: ETable.WhichETableFile, op: Operation,    canTrustLV: BOOLEAN ¬ TRUE, vP: Volume.PageNumber] =    -- lvInfo.*FileHandle gets storage allocated on a READ operation.    -- The FileHeader page allocated here is not mapped to a header page on disk,    -- DoHeaderIO must be called to force the write to the disk page.    -- This proc knows headerSize = 1 and will only have one run.    -- vP kludge: when ETables are first created the startPages are not in    -- the lvRootPage so the client has to pass them to DoHeaderIO.    BEGIN    channel: DiskBackingStore.ChannelHandle;    sv: VolTable.SVDesc;    found: BOOLEAN;    headerBuffer: ETable.FileHeaderHandle;    memPage: Environment.PageNumber;    volPage: Volume.PageNumber;    defaultRunSize: CARDINAL = headerSize;    runs: ARRAY [0..defaultRunSize) OF BackingStore.Run;    diskData: LONG POINTER TO DiskBackingStore.Data ¬      DiskBackingStore.PDiskDataFromPBSData[@runs[0].data];    runs[0].count ¬ headerSize;    -- if op=map then Allocate new headerBuffer.    IF op = read THEN BEGIN      headerBuffer ¬ Space.Allocate[headerSize, !	Space.InsufficientSpace => Bug[outOfVM]].pointer;      VM.ScratchMap[interval:	[Space.PageFromLongPointer[headerBuffer], headerSize]];      lvInfo.fileHandle ¬ headerBuffer;      END;    memPage ¬ Space.PageFromLongPointer[lvInfo.fileHandle];    SELECT which FROM      primary =>	volPage ¬ IF canTrustLV THEN lvInfo.lvHandle.primaryETableStartPage	          ELSE vP;      copy =>	volPage ¬ IF canTrustLV THEN lvInfo.lvHandle.copyETableStartPage	          ELSE vP;      both => Bug[impossibleCase];      ENDCASE => Bug[impossibleCase];    [found, channel] ¬ VolTable.FindSV[lvInfo.lvHandle.vID,      volPage, @sv];    IF ~found THEN Bug[noSuchSV];    WITH d: diskData SELECT FileLock.lockingEnabled FROM      TRUE => d.lock ¬ FileLock.nullLockHandle;      FALSE => d.file ¬ ConvertTypeToID[PilotFileTypesExtraExtras.tETable];      ENDCASE;    diskData.type ¬ PilotFileTypesExtraExtras.tETable;    diskData.channelHandle ¬ channel;    diskData.volumePage ¬ sv.pvPageOfSV + volPage;    [diskData.filePageLow, diskData.filePageHigh] ¬      DiskBackingStore.UnpackFilePageNumber[volPage];    diskData.fileAttributes ¬ [temporary: FALSE, readOnly: FALSE];    SELECT op FROM      read =>        VM.CopyIn[interval: [page: memPage, count: headerSize],	  transferProc: DiskBackingStore.Transfer,	  run: runs[0], returnWait: wait];      write =>         VM.CopyOut[[memPage, headerSize],          DiskBackingStore.Transfer, runs[0], wait];    ENDCASE => Bug[impossibleCase];    END;  -- DoHeaderIO  ETableSizeInWords: PROC [pageGroupCount: CARDINAL]    RETURNS [CARDINAL] = INLINE {    RETURN[      SIZE[ETable.ETableHeader] + (pageGroupCount * SIZE[ETable.PageGroup])];};  ETFileETableCopyOut: PROC [destination, source: LONG POINTER -- to an ETable--,    sourcePageCount: Environment.PageCount] =    -- this LongCopy constrains number of words to move from source.    -- ie. source ETable cannot exceed 256 pages.    BEGIN    VM.MakeResident[      [Space.PageFromLongPointer[source], sourcePageCount], wait];    Inline.LongCOPY[from: source,      nwords: CARDINAL[Environment.wordsPerPage*sourcePageCount],      to: destination];    VM.MakeSwappable[[Space.PageFromLongPointer[source], sourcePageCount]];    VM.ForceOut[      [Space.PageFromLongPointer[destination], sourcePageCount], wait];    END;      FindBucketInfo: PROC [tok: VolTable.LVToken] RETURNS [infoRelPtr: BInfoRelPtr] =    -- at ETable.Open when lvBucketInfo was created this context was set.    INLINE {      VolTable.GetETableContext[tok, @LOOPHOLE[infoRelPtr, VolTable.Context]];      RETURN [infoRelPtr]};                     FindBucketInfoByLVID: PROC [lvID: Volume.ID] RETURNS [infoRelPtr: BInfoRelPtr] =    BEGIN    -- for client who might not have LVToken...gasp. Or to check token context?    FOR infoRelPtr ¬ headOfList, list[infoRelPtr].nextLVBucketInfo      WHILE infoRelPtr ~= nil DO      IF list[infoRelPtr].lvHandle.vID = lvID THEN RETURN[infoRelPtr];    ENDLOOP;    RETURN [nil];    END;  --FindBucketInfoByLVID--  FinishWithSequence: PROCEDURE [pageGroups: GroupSequenceHandle] =    BEGIN    WITH group: pageGroups SELECT FROM      frame => NULL;      createdSpace =>	BEGIN	[] ¬ VM.Unmap[Space.PageFromLongPointer[group.interval.pointer]];	Space.Deallocate[group.interval];	END;      ENDCASE => Bug[impossibleCase];    END;  --FinishWithSequence--  FreeInfo: PROCEDURE [node: BInfoRelPtr] = INLINE    BEGIN    IF ResidentHeap.FreeNode[node].s ~= okay THEN Bug[freeError];    END;      GetBSRuns: PROCEDURE [lvHandle: LVF.Handle, runs: VM.BackingStoreRuns,    window: Window]    RETURNS [newRuns: VM.BackingStoreRuns] =    -- This PROC builds BackingStoreRuns for the file page interval specified by    -- window.  We loop through the page groups of window.etable until the    -- entire interval is captured in runs.    -- If there are more runs than client's runs can hold, we're IN TROUBLE.    BEGIN    OPEN w: window;    volumePage: Volume.PageNumber;    page: File.PageNumber;    pgroup: ETable.PageGroup;    sv: VolTable.SVDesc;    success: BOOLEAN;    i, j: CARDINAL ¬ 0;    countLeft: Volume.PageCount ¬ 0;    dbs: LONG POINTER TO DiskBackingStore.Data;    page ¬ w.base;  -- where does the interval start.    countLeft ¬ w.count;  -- how long the interval is.        UNTIL j = w.etable.header.howManyGroups OR countLeft = 0 DO      [volumePage: volumePage, containingPageGroup: pgroup, index: j] ¬	ETI.WhereIsFilePageOnVolume[page, w.etable];      IF volumePage = ETI.nullETableFilePage THEN        ERROR ETable.Error[pageGroupNotFound];      j ¬ j + 1;      -- adjust count to reflect run starting at volumePage, not pgroup.volumePage      pgroup.count ¬ pgroup.count - CARDINAL[volumePage - pgroup.volumePage];      IF pgroup.count >= countLeft THEN {	runs[i].count ¬ countLeft;	countLeft ¬ 0}      ELSE 	    	runs[i].count ¬ pgroup.count;      -- ok, we just set up runs.count and got volumePage.      -- Now do rest of disk data that appears in a BackingStoreRun.            dbs ¬ DiskBackingStore.PDiskDataFromPBSData[@runs[i].data];      WITH d: dbs SELECT FileLock.lockingEnabled FROM        TRUE => d.lock ¬ FileLock.nullLockHandle;        FALSE => d.file ¬ ConvertTypeToID[PilotFileTypesExtraExtras.tETable];      ENDCASE;      dbs.type ¬ PilotFileTypesExtraExtras.tETable;      dbs.fileAttributes.temporary ¬ FALSE;      [readOnly: dbs.fileAttributes.readOnly] ¬	VolTable.GetLVStatus[lvHandle.vID];      [dbs.filePageLow, dbs.filePageHigh] ¬	DiskBackingStore.UnpackFilePageNumber[volumePage];      [found: success, channel: dbs.channelHandle] ¬ VolTable.FindSV[	lvHandle.vID, volumePage, @sv];      IF ~success THEN Bug[fileSystemInconsistency];      dbs.volumePage ¬	sv.pvPageOfSV + volumePage;      -- All done with this run, now set up for the first page of the next run.      IF countLeft > 0 THEN countLeft ¬ (countLeft - pgroup.count);      page ¬ page + runs[i].count;	        i ¬ i + 1;    ENDLOOP;    IF countLeft ~= 0 THEN Bug[noSuchRun];  -- no more page groups.    newRuns ¬ runs;    END; -- GetBSRuns --      GetLongerSequence: PROC [old: GroupSequenceHandle]    RETURNS [new: GroupSequenceHandle] =    BEGIN    newSize: CARDINAL =  -- allocate a single extra page      WITH o: old SELECT FROM	frame => 1,	createdSpace =>	  IF o.interval.count = LAST[CARDINAL] THEN	    BugV[fileExcessivelyFragmented]	  ELSE CARDINAL[o.interval.count + 1],	ENDCASE => BugV[impossibleCase];    newSpace: Space.Interval =       Space.Allocate[newSize ! Space.InsufficientSpace => Bug[allocationError]];    VM.Map[      interval: [Space.PageFromLongPointer[newSpace.pointer], newSize],      transferProc: NIL, backingStoreRuns: NIL, life: alive,      usage: KernelSpaceUsage.volumeFileMap, swappability: resident,      swapUnits: [unitary[]]];    new ¬ [      DESCRIPTOR[	newSpace.pointer,	(newSize * Environment.wordsPerPage) / SIZE[ETable.PageGroup]],      createdSpace[newSpace]];    Inline.LongCOPY[      from: @old.pageGroups[0],      nwords: LENGTH[old.pageGroups] * SIZE[ETable.PageGroup],      to: @new.pageGroups[0]];    -- The above is equivalent to the following:    -- FOR i: CARDINAL IN [0..LENGTH[old.pageGroups]) DO    --   new.pageGroups[i] ¬ old.pageGroups[i];    --   ENDLOOP;    FinishWithSequence[old];    END;  -- of GetLongerSequence --  GetPages: PROC [startPage: Volume.PageNumber, sizeWanted: Volume.PageCount,    lvHandle: LVF.Handle, tok: VolTable.LVToken,    groupSequence: LONG POINTER TO GroupSequenceHandle] =    -- GetPages MUST take a LONG POINTER TO GroupSequenceHandle because it    -- updates the LENGTH field of groupSequence.pageGroups, which is really    -- a field in the record groupSequence.    BEGIN    currentContents: VM.Interval ¬ VM.nullInterval;    fileD: KernelFile.Descriptor ¬ [	fileID: ConvertTypeToID[PilotFileTypesExtraExtras.tETable],        volumeID: lvHandle.vID, temporary: FALSE,        size: 0, type: PilotFileTypesExtraExtras.tETable];    group, prevGroup: KernelFile.PageGroup;    pageGroupsAllocated: CARDINAL ¬ 0;        prevGroup ¬ [      filePage: 0, volumePage: startPage, nextFilePage: 0];    UNTIL (fileD.size = sizeWanted) DO      BEGIN      group ¬ [	filePage: fileD.size,	volumePage: prevGroup.volumePage + prevGroup.nextFilePage - 	  prevGroup.filePage,	nextFilePage:	  IF sizeWanted - fileD.size > LAST[CARDINAL] THEN	    LAST[CARDINAL] ELSE sizeWanted];	    -- 16 bit word dependency!! See ETable.PageGroup      VolAllocMap.AllocPageGroup[	vol: lvHandle, filePtr: @fileD, groupPtr: @group,	data: currentContents, tok: tok !	Volume.InsufficientSpace => GO TO Undo];	-- results in modified group and FilePtr.size      IF pageGroupsAllocated >= LENGTH[groupSequence.pageGroups] THEN        groupSequence­ ¬ GetLongerSequence[groupSequence­];      groupSequence.pageGroups[pageGroupsAllocated] ¬ [	count: CARDINAL[group.nextFilePage - group.filePage],	volumePage: group.volumePage];      pageGroupsAllocated ¬ pageGroupsAllocated + 1;      prevGroup ¬ group;      END;      REPEAT	Undo => {	-- ASSUMES no labels, just setting VAM!	  FOR i: CARDINAL IN [0..pageGroupsAllocated) DO	    VolAllocMap.SetPageGroup[	      lvHandle, groupSequence.pageGroups[i].volumePage,	      groupSequence.pageGroups[i].count, free, ignorePrevSetting, tok];	    ENDLOOP;	  FinishWithSequence[groupSequence­];	  ERROR Volume.InsufficientSpace[lvHandle.freePageCount, lvHandle.vID]};      ENDLOOP;      groupSequence.pageGroups.LENGTH ¬ pageGroupsAllocated;    END;  -- of GetPages --    GrowOverflow: PUBLIC --ETable-- PROC [lvBucketInfo: ETable.LVBucketInfo,    count: File.PageCount]    RETURNS [countDone: LONG CARDINAL] =    BEGIN    freePages, pagesForET: Volume.PageCount;    startPage: Volume.PageNumber;    defaultGroupCount: CARDINAL = 10;    defaultPageGroups: ARRAY [0..defaultGroupCount) OF ETable.PageGroup;    groupSequence: GroupSequenceHandle ¬      [DESCRIPTOR[defaultPageGroups], frame[]];    pgCount: CARDINAL;  -- page groups in an etable.    changingET: ETable.ETableHandle;    IF (count MOD 4) # 0 THEN      count ¬ count + (4 - (count MOD 4));    IF (freePages ¬ lvBucketInfo.lvHandle.freePageCount) <= (2*count)    THEN {      IF (freePages ¬ lvBucketInfo.lvHandle.freePageCount) <=         (2*ETI.overflowBufferSize) THEN        ERROR Volume.InsufficientSpace[freePages, lvBucketInfo.lvHandle.vID]      ELSE count ¬ ETI.overflowBufferSize;};    FOR which: ETable.WhichETableFile IN [primary..both) DO BEGIN      SELECT which FROM	primary => {	  changingET ¬ lvBucketInfo.primaryETableHandle;	  -- Lets log both ETableFile ETables now.  Order is important.	  -- Logging machinery is also going to look at lvBucketInfos ETables 	  -- to derive Volume.PageNumbers.	  Logger[preImage: changingET,	    count: lvBucketInfo.fileHandle.maxETableSize,	    lvInfo: lvBucketInfo];	  Logger[preImage: lvBucketInfo.copyETableHandle,	    count: lvBucketInfo.fileHandle.maxETableSize,	    lvInfo: lvBucketInfo];	  lvBucketInfo.fileHandle.itemLogged ¬ eTFET;	  lvBucketInfo.fileHandle.firstFilePageLogged ¬	    headerSize + lvBucketInfo.fileHandle.preImageSize;	  DoHeaderIO[lvBucketInfo, primary, write,,];	  DoHeaderIO[lvBucketInfo, copy, write,,];	  startPage ¬ ETI.WhereIsFilePageOnVolume[	    ETI.GetSize[changingET], changingET].volumePage;};	copy => {	  changingET ¬ lvBucketInfo.copyETableHandle;	  startPage ¬ ETI.WhereIsFilePageOnVolume[	    ETI.GetSize[changingET], changingET].volumePage;};	both => Bug[impossibleCase];	ENDCASE => Bug[impossibleCase];            GetPages[startPage, count, lvBucketInfo.lvHandle, lvBucketInfo.token,        @groupSequence];      pgCount ¬ changingET.header.howManyGroups;      pagesForET ¬ (ETableSizeInWords[pgCount +        LENGTH[groupSequence.pageGroups]]/Environment.wordsPerPage) + 1;      IF pagesForET > lvBucketInfo.fileHandle.maxETableSize THEN        ERROR ETable.Error[tooFragmented];      VM.MakeWritable[[Space.PageFromLongPointer[changingET],        lvBucketInfo.fileHandle.maxETableSize]];      Inline.LongCOPY[        from: @groupSequence.pageGroups[0],	nwords: LENGTH[groupSequence.pageGroups] * SIZE[ETable.PageGroup],	to: @changingET.pageGroups[pgCount]];      -- The above is equivalent to the following:      -- FOR i: CARDINAL IN [0..LENGTH[groupSequence.pageGroups]) DO      --   changingET.pageGroups[pgCount + i] ¬ groupSequence.pageGroups[i];      --   ENDLOOP;      changingET.header.howManyGroups ¬        pgCount + LENGTH[groupSequence.pageGroups];      FinishWithSequence[groupSequence];  -- free groupSequence      VM.ForceOut[	[Space.PageFromLongPointer[changingET],	 lvBucketInfo.fileHandle.maxETableSize],	 wait];      VM.MakeReadOnly[[Space.PageFromLongPointer[changingET],         lvBucketInfo.fileHandle.maxETableSize]];      END;    ENDLOOP;    lvBucketInfo.fileHandle.overflowSize ¬      lvBucketInfo.fileHandle.overflowSize + count;     DoHeaderIO[lvBucketInfo, primary, write,,];    DoHeaderIO[lvBucketInfo, copy, write,,];    CopyPrimariesToCopies[lvBucketInfo.token, lvBucketInfo.lvHandle,,,];    ClearLog[lvBucketInfo];    RETURN[count];    END;  -- of GrowOverflow --          Logger: PROC [preImage: LONG POINTER,     count: File.PageCount, lvInfo: ETable.LVBucketInfo] =    BEGIN    -- There is a Log region associated with each ETableFile. We have to copy    -- data from preImage into each of these logs.    logFilePage, imageMemPage: Environment.PageNumber;    headerPage: ETable.FileHeaderHandle ¬ lvInfo.fileHandle;    etfETable: ETable.ETableHandle;    countLogged: CARDINAL ¬ 0;    log: ImageLogHandle;    FOR which: ETable.WhichETableFile IN [primary..copy] DO      SELECT which FROM      primary => {	etfETable ¬ lvInfo.primaryETableHandle};      copy => {	etfETable ¬ lvInfo.copyETableHandle};      ENDCASE => Bug[impossibleCase];      -- will new entries fit?      IF (headerPage.numberOfPagesLogged + count) > headerPage.preImageSize        THEN ERROR ETable.Error[logFull];      log ¬ LOOPHOLE[Space.Allocate[headerPage.preImageSize !         Space.InsufficientSpace => Bug[outOfVM]].pointer];      MapRuns[        lvInfo.lvHandle, log, [etfETable, headerSize, headerPage.preImageSize]];      -- determine page for next entry      logFilePage ¬	Space.PageFromLongPointer[log] + headerPage.numberOfPagesLogged;      imageMemPage ¬ Space.PageFromLongPointer[preImage];      countLogged ¬ 0;      WHILE countLogged < count DO	-- copy preImage page into log	Inline.LongCOPY[from: Space.LongPointerFromPage[imageMemPage],	  nwords: Environment.wordsPerPage,	  to: Space.LongPointerFromPage[logFilePage]];	logFilePage ¬ logFilePage.SUCC;	imageMemPage ¬ imageMemPage.SUCC;	countLogged ¬ countLogged.SUCC;	ENDLOOP;  -- WHILE countLogged < count --      -- VM.ForceOut log interval      VM.ForceOut[        [Space.PageFromLongPointer[log], headerPage.preImageSize], wait];      UnmapImageLog[headerPage, log];      ENDLOOP;  -- FOR which: ETable.WhichETableFile --    -- update numberOfPagesLogged    headerPage.numberOfPagesLogged ¬      headerPage.numberOfPagesLogged + countLogged;    END;  -- of Logger --    MakeNewInfo: PROCEDURE [size: Zone.BlockSize]    RETURNS [node: BInfoRelPtr --ListBase RELATIVE POINTER--] = INLINE    -- allocates an lvBucketInfoData record from the heap.    BEGIN    status: Zone.Status;    [node, status] ¬ ResidentHeap.MakeNode[size];    IF status ~= okay THEN Bug[allocationError];    END;  MapBucketHandle: PROC [lvBucketInfo: ETable.LVBucketInfo] =    BEGIN    lvBucketInfo.bucketHandle ¬ LOOPHOLE[Space.Allocate[ETI.normalBucketSize !      Space.InsufficientSpace => Bug[outOfVM]].pointer];    VM.ScratchMap[      [Space.PageFromLongPointer[lvBucketInfo.bucketHandle],      ETI.normalBucketSize]];    END;    MapETFETable: PROC [    lvInfo: ETable.LVBucketInfo, which: ETable.WhichETableFile] =    -- lvInfo.*ETableHandle should be NIL on entry to this proc.    -- We Allocate space for it here.    -- ETF Header pages must be available through lvInfo.    -- lvInfo.*ETableHandle gets MAPPED directly to disk pages.    -- First we CopyIn the first page of an ETFETable and ASSUME that it contains    -- page groups to describe the rest of itself. temp is scratch because we need    -- to smash howManyPageGroups in header before passing it around.    -- We pass this first page to MapRuns which maps all of etfET.    BEGIN    pagesForETable: Environment.PageCount;    defaultRunSize: CARDINAL = 1;  -- want this to be 1, see ASSUMPTION below.    etfET, temp: ETable.ETableHandle;    sv: VolTable.SVDesc;    channel: DiskBackingStore.ChannelHandle;    found: BOOLEAN;    defaultRuns: ARRAY [0..defaultRunSize) OF BackingStore.Run;    runs: VM.BackingStoreRuns ¬ DESCRIPTOR[defaultRuns];    dData: LONG POINTER TO DiskBackingStore.Data ¬      DiskBackingStore.PDiskDataFromPBSData[@runs[0].data];    volumePage: Volume.PageNumber;    etFilePage: File.PageNumber;    pGroupsOnFirstPage: CARDINAL ¬ (Environment.wordsPerPage -       SIZE[ETable.ETableHeader])/SIZE[ETable.PageGroup];    runs[0].count ¬ defaultRunSize;    pagesForETable ¬ lvInfo.fileHandle.maxETableSize;    etfET ¬ LOOPHOLE[Space.Allocate[pagesForETable !      Space.InsufficientSpace => Bug[outOfVM]].pointer];    SELECT which FROM      primary =>       {volumePage ¬ lvInfo.fileHandle.primaryETFHeader.myFirstETablePage;	etFilePage ¬ headerSize + lvInfo.fileHandle.preImageSize;	lvInfo.primaryETableHandle ¬ etfET};      copy =>       {volumePage ¬ lvInfo.fileHandle.copyETFHeader.myFirstETablePage;	etFilePage ¬	  headerSize + pagesForETable + lvInfo.fileHandle.preImageSize;	lvInfo.copyETableHandle ¬ etfET};      both => Bug[impossibleCase];      ENDCASE => Bug[impossibleCase];    temp ¬ LOOPHOLE[Space.Allocate[1, !      Space.InsufficientSpace => Bug[outOfVM]].pointer];    VM.ScratchMap[      [Space.PageFromLongPointer[temp], 1]];    [found, channel] ¬ VolTable.FindSV[lvInfo.lvHandle.vID, volumePage, @sv];    IF ~found THEN Bug[noSuchSV];    WITH d: dData SELECT FileLock.lockingEnabled FROM      TRUE => d.lock ¬ FileLock.nullLockHandle;      FALSE => d.file ¬ ConvertTypeToID[PilotFileTypesExtraExtras.tETable];      ENDCASE;    dData.type ¬ PilotFileTypesExtraExtras.tETable;    dData.channelHandle ¬ channel;    dData.volumePage ¬ sv.pvPageOfSV + volumePage;    [dData.filePageLow, dData.filePageHigh] ¬      DiskBackingStore.UnpackFilePageNumber[volumePage];    dData.fileAttributes ¬ [temporary: FALSE, readOnly: TRUE];    VM.CopyIn[      interval: [page: Space.PageFromLongPointer[temp], count: 1],      transferProc: DiskBackingStore.Transfer,      run: runs[0], returnWait: wait];    -- Map entire ETable area.    -- ASSUMPTION: All page groups needed are on single temp page just CopiedIn!    -- We're mapping the entire area not just that portion occupied by page    -- groups.    temp.header.howManyGroups ¬       MIN[temp.header.howManyGroups, pGroupsOnFirstPage];    MapRuns[lvInfo.lvHandle, etfET,      [etable: temp, base: etFilePage, count: pagesForETable]];    VM.MakeReadOnly[      [Space.PageFromLongPointer[etfET], pagesForETable]];    VM.Unmap[Space.PageFromLongPointer[temp]];    Space.Deallocate[[temp, 1]];    END;  -- of MapETFETable--       MapLVRootPage: PROC [lvBucketInfo: ETable.LVBucketInfo] = INLINE {    lvBucketInfo.lvHandle ¬ VolTable.GetLVHandle[lvBucketInfo.token]};  MapOverflowWindow: PROC [lvBucketInfo: ETable.LVBucketInfo, count: CARDINAL] = {    lvBucketInfo.overflowHandle ¬ LOOPHOLE[Space.Allocate[count      ! Space.InsufficientSpace => Bug[outOfVM]].pointer];    VM.ScratchMap[[      Space.PageFromLongPointer[lvBucketInfo.overflowHandle], count]]};    MapRuns: PROC [lvHandle: LVF.Handle, mapThis: LONG POINTER, toThis: Window] =    -- mapThis must be an interval big enough to map the pages described in    -- toThis. toThis.etable must of course be mapped, and should contain page    -- groups for all the pages to be mapped.  Client may SetAccess of mapThis    -- upon return.    BEGIN    i, count: CARDINAL ¬ 0;    defaultRunSize: CARDINAL = 10;    defaultRuns: ARRAY [0..defaultRunSize) OF BackingStore.Run;    runs: VM.BackingStoreRuns ¬ DESCRIPTOR[defaultRuns];    runs ¬ GetBSRuns[      lvHandle: lvHandle,      runs: runs,      window: [etable: toThis.etable, base: toThis.base, count: toThis.count]];    VM.Map[      interval: [Space.PageFromLongPointer[mapThis], toThis.count],      transferProc: DiskBackingStore.Transfer, swapUnits: [unitary[]],       backingStoreRuns: runs,      usage: KernelSpaceUsage.volumeFileMap, access: readWrite];    END;  MinPagesForETables: PUBLIC --FileBasicsPrograms-- PROCEDURE [Volume.PageCount]    RETURNS [Volume.PageCount] = {RETURN[LONG[100]]};        Open: PUBLIC --ETable-- ENTRY PROCEDURE [token: VolTable.LVToken] =    BEGIN ENABLE UNWIND => NULL;    -- Open: called from Volume.Open, first thing to do is call Verifier.    -- Allocate new lvBucketInfo record from ResidentHeap and map various    -- buffers/structures it points to.    -- LVEntry state is open (or we wouldn't be here) so rootpage is readable.    -- lvRootPage is mapped and readable.    -- NEVER called if volume already Open.    -- We don't yet know how many pages are needed for:    --	   ETableFileETables    --	   preImageLog    -- This info is in ETableFileHeader which we don't    -- know validity of yet,    -- Are these buffers backed by disk (i/o with VM.Map) or backed by    -- resident memory (i/o with VM.CopyIn/Out)?        GetOverflowFreeSpace: PROC RETURNS [fS: ETable.FreeSpaceInOverflow] =      BEGIN      defaultRunSize: CARDINAL = 1;      sv: VolTable.SVDesc;      channel: DiskBackingStore.ChannelHandle;      found: BOOLEAN;      defaultRuns: ARRAY [0..defaultRunSize) OF BackingStore.Run;      runs: VM.BackingStoreRuns ¬ DESCRIPTOR[defaultRuns];      dData: LONG POINTER TO DiskBackingStore.Data ¬	DiskBackingStore.PDiskDataFromPBSData[@runs[0].data];      runs[0].count ¬ defaultRunSize;      [found, channel] ¬ VolTable.FindSV[        lvInfo.lvHandle.vID, lvInfo.fileHandle.primaryETFHeader.firstOverflowPage, @sv];      IF ~found THEN Bug[noSuchSV];      WITH d: dData SELECT FileLock.lockingEnabled FROM        TRUE => d.lock ¬ FileLock.nullLockHandle;	FALSE => d.file ¬ ConvertTypeToID[PilotFileTypesExtraExtras.tETable];	ENDCASE;      dData.type ¬ PilotFileTypesExtraExtras.tETable;      dData.channelHandle ¬ channel;      dData.volumePage ¬ sv.pvPageOfSV +         lvInfo.fileHandle.primaryETFHeader.firstOverflowPage;      [dData.filePageLow, dData.filePageHigh] ¬       	DiskBackingStore.UnpackFilePageNumber[	  lvInfo.fileHandle.primaryETFHeader.firstOverflowPage];      dData.fileAttributes ¬ [temporary: FALSE, readOnly: TRUE];      VM.CopyIn[	interval: [Space.PageFromLongPointer[lvInfo.bucketHandle], 1],	transferProc: DiskBackingStore.Transfer, 	run: runs[0], returnWait: wait];      fS ¬ lvInfo.bucketHandle.header.freeSpace;      END;  -- of GetOverflowFreeSpace    lvInfo: ETable.LVBucketInfo;    lvInfo ¬ @list[MakeNewInfo[SIZE[ETable.LVBucketInfoData]]];    lvInfo.token ¬ token;                 MapLVRootPage[lvInfo]; -- either map it here or share LVToken's handle    --  order here is IMPORTANT!    DoHeaderIO[lvInfo, primary, read,,];  -- CopyIn lvInfo.fileHandle    MapETFETable[lvInfo, primary];  -- maps lvInfo.primaryETableHandle    MapETFETable[lvInfo, copy];  -- actually maps pages of copy ETF    MapBucketHandle[lvInfo];  -- maps lvInfo.bucketHandle    MapOverflowWindow[lvBucketInfo: lvInfo, count: ETI.overflowBufferSize];    lvInfo.currentBucketPage ¬ ETI.nullETableFilePage;    lvInfo.fileID ¬ File.nullID;    lvInfo.eTableHandle ¬ NIL;    lvInfo.inBucket ¬ FALSE;    lvInfo.currentOverflowPage ¬ ETI.nullETableFilePage;    lvInfo.freeSpaceInOverflow ¬ GetOverflowFreeSpace[];        lvInfo.hash ¬ lvInfo.fileHandle.numberOfBuckets;    lvInfo.firstBucketPage ¬      headerSize + lvInfo.fileHandle.preImageSize +      (2*lvInfo.fileHandle.maxETableSize);    lvInfo.firstOverflowPage ¬      lvInfo.firstBucketPage + lvInfo.fileHandle.numberOfBuckets;    lvInfo.preImageLogHandle ¬ NIL;    lvInfo.token ¬ nilLVT;    lvInfo.nextLVBucketInfo ¬ nil;     IF headOfList = nil THEN     headOfList ¬ firstOpened ¬ BInfoRelFromPtr[lvInfo]     ELSE {      lvInfo.nextLVBucketInfo ¬ headOfList;      headOfList ¬ BInfoRelFromPtr[lvInfo]};    VolTable.SetETableContext[tok: token, context: LOOPHOLE[headOfList]];    END;  -- Open --  ReleaseLVBucketInfo: PUBLIC --ETableInternal-- ENTRY PROC [lvBI: ETable.LVBucketInfo]    RETURNS [ETable.LVBucketInfo] =    -- internal client is done with current operation that needed exclusive access    -- to lvBucketInfo    BEGIN    infoRelPtr: BInfoRelPtr ¬ BInfoRelFromPtr[lvBI];    IF list[infoRelPtr].token = nilLVT THEN      RETURN WITH ERROR ETable.Error[invalidBucketInfo];    list[infoRelPtr].token ¬ nilLVT;    NOTIFY lvInfoUnlocked;  -- only head WAITer need be notified, why tell everybody.    RETURN[NIL];    END;  UnmapHeader: PROC [lvBucketInfo: ETable.LVBucketInfo] =    BEGIN      VM.Unmap[Space.PageFromLongPointer[lvBucketInfo.fileHandle]];      Space.Deallocate[[lvBucketInfo.fileHandle, headerSize]];      lvBucketInfo.fileHandle ¬ NIL;    END;  UnmapETFETables: PROC [lvBucketInfo: ETable.LVBucketInfo] = {};  UnmapImageLog: PROC [headerPage: ETable.FileHeaderHandle, log: ImageLogHandle] =    INLINE BEGIN    VM.Unmap[Space.PageFromLongPointer[log]];    Space.Deallocate[[log, headerPage.preImageSize]];    END;  UnmapLVRootPage: PROC [lvBucketInfo: ETable.LVBucketInfo] = INLINE {    lvBucketInfo.lvHandle ¬ NIL;};    -- no space to clean up here 'cause storage owned by VolTable.   UnmapOverflowWindow: PROC [lvBucketInfo: ETable.LVBucketInfo] =    BEGIN    VM.Unmap[Space.PageFromLongPointer[lvBucketInfo.overflowHandle]];    Space.Deallocate[[lvBucketInfo.overflowHandle, ETI.overflowBufferSize]];    lvBucketInfo.overflowHandle ¬ NIL;    END;    WriteBucketHeaders: PROC [bucketBuf: ETable.BucketHandle,    currentPage, lastPage: File.PageNumber,    lvHandle: LVF.Handle,    etable: ETable.ETableHandle,    writeOverflow: BOOLEAN, firstOverflow: Volume.PageNumber] =    -- WriteBucketHeaders: is passed a single page buffer which is used to write    -- BucketHeaders to the specified range of pages.  This is done by getting    -- page groups from etable, and calling DiskChannel.DoIO using these runs.    -- This PROC is called from CreateFile and AddBuckets.    -- It Deallocates bucketBuf when it's done.    BEGIN    pgroup: ETable.PageGroup;    volumePage: Volume.PageNumber;    channel: DiskBackingStore.ChannelHandle;    dcChannel: DiskChannel.Handle;    found: BOOLEAN;    sv: VolTable.SVDesc;    request: DiskChannel.IORequest ¬ [      diskPage: firstOverflow, -- safe page if accident.      memoryPage: Space.PageFromLongPointer[bucketBuf],      tries: DiskChannel.defaultTries,      count: 0, useSamePage: TRUE,      command: write];      bucketBuf.header.eTablesInOverflowCount ¬ 0;    bucketBuf.header.firstFree ¬ LOOPHOLE[CARDINAL[SIZE[ETable.BucketHeader]]];    [found, channel] ¬ VolTable.FindSV[lvHandle.vID,      ETI.WhereIsFilePageOnVolume[1, etable].volumePage, @sv];      -- page one is close enough.    IF ~found THEN Bug[noSuchSV];    dcChannel ¬ DiskBackingStore.GetDiskChannel[channel];    WHILE currentPage <= lastPage DO      -- ASSERT: currentPage does not have a header yet.      [volumePage: volumePage, containingPageGroup: pgroup] ¬	ETI.WhereIsFilePageOnVolume[currentPage, etable];      -- adjust count to reflect run starting at volumePage, not pgroup.volumePage      pgroup.count ¬ pgroup.count - CARDINAL[volumePage - pgroup.volumePage];      IF currentPage + pgroup.count - 1 > lastPage THEN	pgroup.count ¬ CARDINAL[lastPage - currentPage] + 1;      currentPage ¬ currentPage + pgroup.count;      request.diskPage ¬ sv.pvPageOfSV + volumePage;      request.count ¬ pgroup.count;      IF DiskChannel.DoIO[dcChannel, @request].status ~=	DiskChannel.goodCompletion THEN ERROR ETable.Error[unexpectedDiskError];    ENDLOOP;  -- all bucket headers written.    IF writeOverflow THEN BEGIN      -- write overflow header      bucketBuf.header.freeSpace ¬ LOOPHOLE[LONG[SIZE[ETable.BucketHeader]]];      request.diskPage ¬ sv.pvPageOfSV + firstOverflow;      request.count ¬ 1;      IF DiskChannel.DoIO[dcChannel, @request].status ~=	DiskChannel.goodCompletion THEN ERROR ETable.Error[unexpectedDiskError];      END;    VM.Unmap[Space.PageFromLongPointer[bucketBuf]];    Space.Deallocate[[bucketBuf, 1]];    END;  -- of WriteBucketHeaders  END...LOG.18-Jul-86 17:20:53   CJS  Really go no Labels.22-Jul-86 11:38:25   CJS  Implemented ClearLog.23-Jul-86 13:01:46   CJS  Fixed LongCOPY loop in Logger and headerBuffer in DoHeaderIO. MapETFETable and GetOverflowFreeSpace runs[0].count ¬ 1.  Also fixed GrowOverflow loop for copying into ETable file's ETables.11-Aug-86 15:16:08   RSV  Changed ETable.Error[insufficientSpace] to Volume.InsufficientSpace[currentFreeSpace, volume].  Also made changes to GetPages for VolAllocMap peculiarity.  Zeroed out hdr.numberOfPagesLogged in CreateInternal. 2-Sep-86  9:39:02   RSV  Made things pretty and alphabetized.  Moved MakeFileList to ETableImplC. 5-Sep-86 10:34:43   CJS  runs[0].count ¬ defaultRunSize a couple places. Use ETable.PageGroupSeq and define defaultGroupCount where needed.15-Sep-86 16:13:29   RSV/CJS  Added temporary logger hacks.  Added checking (and saving) status returned from ResidentHeap calls.17-Sep-86  9:51:55   RSV  Made change of runs[0].count ¬ defaultRunSize in procedure GetOverflowFreeSpace.22-Sep-86 15:04:25   CJS  Change Logger usage in AddBuckets and GrowOverflow, do itemLogged and firstFilePageLogged more correctly.22-Oct-86  8:31:17   CJS/RSV  ETable.FileHeader format change. fileHandle in LVBucketInfoData replaces copyFileHandle and primaryFileHandle. Also replaced some incorrect ForceOuts with DoHeaderIO. CopyOut ETFHeaders in CreateFile after both ETableFiles are created. Get order of ETFETables correct on disk! Update numberOfPagesLogged only once in Logger.29-Oct-86 14:59:48   CJS  Yet another tweak to account for changing order of ETFETables. This one in CopyPrimariesToCopies.17-Nov-86 14:36:06   RSV  Changes for new DiskBackingStore.11-Jan-87 16:17:28   RRR  Unknown changes. 3-Feb-87 10:08:48   CJS  FileBasicsPerf vars.31-Aug-87 13:57:02   RSV  Fixed GetBSRuns to set runs[i].count correctly if countLeft is small.28-Jun-88 10:45:18   RSV  Changed AddBuckets, CreateFile, GetLongerSequence, GetPages, and GrowOverflow because of ETable.PageGroupSeq and ETable.PageGroupHandle changing to be ARRAYs and DESCRIPTORs allocated from Space or the local frame, not from the ResidentHeap.  Use Inline.LongCOPY instead of FOR loops.29-Jul-88 18:58:59   RSV  Fixes to disable file locking upon request.