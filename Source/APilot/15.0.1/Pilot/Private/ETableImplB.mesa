-- Copyright (C) 1986, 1987, 1988 by Xerox Corporation. All rights reserved. -- ETableImplB.mesa       1-Dec-88 11:59:51 by RSVDIRECTORY  BackingStore USING [Run],  DiskBackingStore USING [    ChannelHandle, Data, PDiskDataFromPBSData, Transfer, UnpackFilePageNumber],  Environment USING [Base, PageCount, PageFromLongPointer, wordsPerPage],  ETable USING [    BucketHandle, BucketHeader, Error, ETable, ETableHandle, ETableHeader,     FileHeaderHandle, GrowOverflow, LVBucketInfo, PageGroup, PageGroupPtr,    PageGroupHandle, nullPageGroup],  ETableInternal USING [    AcquireLVBucketInfo, ConvertTypeToID, fileHeaderSize, maxPageGroupSize,    normalBucketSize, nullETableFilePage, Operation, overflowBufferSize,    ReleaseLVBucketInfo],  File USING [ID, nullID, PageCount, PageNumber, Type],  FileLock USING [lockingEnabled, nullLockHandle],  Inline USING [LongCOPY, LowHalf],  KernelFile USING [PageGroup],  PilotFileTypesExtraExtras USING [tETable],  ResidentHeap USING [first64K, FreeNode, MakeNode],  RuntimeInternal USING [Bug],  VM USING [CopyIn, CopyOut, Interval],  VolTable USING [FindSV, GetLVStatus, LVToken, SVDesc],  Volume USING [InsufficientSpace, PageNumber],  Zone USING [Status];ETableImplB: PROGRAM  IMPORTS    DiskBackingStore, Environment, ETable, ETableInternal, FileLock, Inline,    ResidentHeap, RuntimeInternal, VM, VolTable, Volume  EXPORTS ETable, ETableInternal =  BEGIN    << Assumptions about lvBucketInfo:     Once acquired, lvBucketInfo has the correct volume information for the fields:      lvHandle, token, hash, fileHandle, primaryETableHandle, and copyETableHandle.      lvBucketInfo.bucketHandle points to a resident one page buffer.      lvBucketInfo.overflowHandle points to a resident overflowBufferSize buffer.     This module sets the lvBucketInfo fields:       currentBucketPage, fileID, inBucket, currentOverflowPage, freeSpaceInOverflow.     And it copy in's and copy out's to bucketHandle and/or overflowHandle  >>       << UNRESOLVED OR UNIMPLEMENTED ISSUES     1) readonly-ness for VM copy ins and outs? Doesn't LVAccess test for it? Ask CJS.     2) Changes for Bob (EX:CreateETableInternal with boolean to not test for duplicate files)  >>  -- DONT always ignore cached fileID in FindETable!!    -- globals      maxEntryInBucket:    ETable.BucketHandle RELATIVE ORDERED POINTER TO ETable.ETable ¬     LOOPHOLE[Environment.wordsPerPage];  idHit: LONG CARDINAL ¬ 0;  -- updated in FindETable.  blockSize: CARDINAL ¬ ETableInternal.overflowBufferSize * Environment.wordsPerPage;      Bug: PROCEDURE [bugType: BugType] = {RuntimeInternal.Bug[bugType]};  BugType: TYPE = {    bucketNotFound, fileWentAway, noPageGroupsRemaining, noSuchSV,    overflowBlockNotFound, readOnlyVolume,    unableToAllocateResidentHeapNode, unableToFreeResidentHeapNode};      -- Procedures --    AddToOverflow: PUBLIC PROCEDURE [    lvBucketInfo: ETable.LVBucketInfo, eTable: ETable.ETableHandle] =    BEGIN    -- Adds the etable to the overflow, growing the overflow if necessary, and    -- nulling out an end of block if the etable can't fit there.    -- Upon completion lvBucketInfo.currentOverflowPage points to the firstOverflowPage.        eTableSize: CARDINAL;    freeSpaceInBlock, oldFreeSpaceInBlock:      ETable.BucketHandle RELATIVE ORDERED POINTER TO ETable.ETable;    whichBlock: LONG CARDINAL;    nullOutEndOfLastBlock: BOOLEAN ¬ FALSE;        -- find the block with the free space in it    freeSpaceInBlock ¬ LOOPHOLE[Inline.LowHalf[LOOPHOLE[      lvBucketInfo.freeSpaceInOverflow, LONG CARDINAL] MOD blockSize]];    whichBlock ¬ LOOPHOLE[lvBucketInfo.freeSpaceInOverflow, LONG CARDINAL]/blockSize;    -- will the etable fit in this block    eTableSize ¬ GetETableSize[eTable];    IF eTableSize > blockSize THEN ETable.Error[tooFragmented];    IF (LOOPHOLE[freeSpaceInBlock, CARDINAL] + eTableSize  + SIZE[ETable.ETableHeader])        > blockSize THEN       BEGIN      -- nope, it don't fit, so put it in the next block.       whichBlock ¬ whichBlock + 1;      oldFreeSpaceInBlock ¬ freeSpaceInBlock;      freeSpaceInBlock ¬ LOOPHOLE[0];      nullOutEndOfLastBlock ¬ TRUE;      IF ((lvBucketInfo.fileHandle.overflowSize /	ETableInternal.overflowBufferSize) = whichBlock) THEN	IF ETable.GrowOverflow[	 lvBucketInfo, ETableInternal.overflowBufferSize] = 0 THEN	 ERROR Volume.InsufficientSpace[lvBucketInfo.lvHandle.freePageCount, 	   lvBucketInfo.lvHandle.vID];      END;    -- so now we know what block the etable goes in, so read it in if necessary    IF lvBucketInfo.currentOverflowPage ~=        (whichBlock * ETableInternal.overflowBufferSize +       lvBucketInfo.firstOverflowPage) THEN {       lvBucketInfo.currentOverflowPage ¬           whichBlock * ETableInternal.overflowBufferSize + lvBucketInfo.firstOverflowPage;       DoOverflowIO[lvBucketInfo, read]};    -- write etable to freeSpace    lvBucketInfo.eTableHandle ¬ @lvBucketInfo.overflowHandle[freeSpaceInBlock];    lvBucketInfo.freeSpaceInOverflow ¬      (whichBlock * blockSize) + freeSpaceInBlock + eTableSize;    Inline.LongCOPY[      from: eTable, nwords: eTableSize, to: lvBucketInfo.eTableHandle];    IF whichBlock = 0 THEN       lvBucketInfo.overflowHandle.header.freeSpace ¬ lvBucketInfo.freeSpaceInOverflow;    DoOverflowIO[lvBucketInfo, write]; -- got etable and possibly header     IF nullOutEndOfLastBlock THEN       BEGIN      -- make a null entry at the end of previous block      freeArea: ETable.ETableHandle;       lvBucketInfo.currentOverflowPage ¬ 	(whichBlock - 1) * ETableInternal.overflowBufferSize +        lvBucketInfo.firstOverflowPage;      DoOverflowIO[lvBucketInfo, read];      freeArea ¬ @lvBucketInfo.overflowHandle[oldFreeSpaceInBlock];       freeArea.header.fileID ¬ File.nullID;      freeArea.header.length ¬ blockSize - LOOPHOLE[oldFreeSpaceInBlock,CARDINAL];      DoOverflowIO[lvBucketInfo, write];      END;    -- update freespace in header    IF whichBlock # 0 THEN { -- we already got the header if we were in block 0 --      lvBucketInfo.currentOverflowPage ¬ lvBucketInfo.firstOverflowPage;      DoOverflowIO[lvBucketInfo, read];      lvBucketInfo.overflowHandle.header.freeSpace ¬ lvBucketInfo.freeSpaceInOverflow;      DoOverflowIO[lvBucketInfo, write]};    END;      CreateETable: PUBLIC PROCEDURE [    token: VolTable.LVToken, fileID: File.ID, pageGroups: ETable.PageGroupHandle,     type: File.Type] =    BEGIN    lvBucketInfo: ETable.LVBucketInfo ¬ ETableInternal.AcquireLVBucketInfo[token];    CreateETableInternal[      lvBucketInfo, fileID, pageGroups, type !         ETable.Error, Volume.InsufficientSpace =>           lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo]];    lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo];    END;      CreateETableInternal: PUBLIC -- ETableInternal -- PROCEDURE [    lvBucketInfo: ETable.LVBucketInfo, fileID: File.ID,    pageGroups: ETable.PageGroupHandle, type: File.Type] =    BEGIN    -- Creates an etable for the file described by fileID, pageGroups, and type.    -- It will be placed in the bucket if the number of page groups doesn't exceed    -- the max, and if the etable fits. Otherwise, it gets thrown in the overflow.    -- lvBucketInfo does NOT contain valid etable information upon completion.            SetUpETable: PROCEDURE [eTable: ETable.ETableHandle] =      BEGIN      eTable.header.fileID ¬ fileID;      eTable.header.temporary ¬ TRUE;      eTable.header.bootable ¬ FALSE;      eTable.header.type ¬ type;      eTable.header.howManyGroups ¬ LENGTH[pageGroups];      Inline.LongCOPY[        from: @pageGroups[0],	nwords: LENGTH[pageGroups] * SIZE[ETable.PageGroup],	to: @eTable.pageGroups[0]];      -- This is equivalent to:      -- FOR i: CARDINAL IN [0..LENGTH[pageGroups]) DO      --   eTable.pageGroups[i] ¬ pageGroups[i];      --   ENDLOOP;      END;          eTableSize: CARDINAL;    FindETable[lvBucketInfo, fileID];     IF lvBucketInfo.eTableHandle # NIL THEN ERROR ETable.Error[duplicateFile];     eTableSize ¬       SIZE[ETable.ETableHeader] + (SIZE[ETable.PageGroup] *      LENGTH[pageGroups]);    IF (LENGTH[pageGroups] <= ETableInternal.maxPageGroupSize) AND         ((eTableSize + lvBucketInfo.bucketHandle.header.firstFree) <=         maxEntryInBucket) THEN       BEGIN -- place it in the bucket       lvBucketInfo.eTableHandle ¬ LOOPHOLE[	@lvBucketInfo.bucketHandle[lvBucketInfo.bucketHandle.header.firstFree]];      SetUpETable[lvBucketInfo.eTableHandle];      lvBucketInfo.bucketHandle.header.firstFree ¬ 	lvBucketInfo.bucketHandle.header.firstFree + eTableSize;      DoBucketIO[lvBucketInfo, write]; -- writes to copy before primary etablefile      END    ELSE BEGIN  -- place it in overflow      newETable: ETable.ETableHandle;      node: Environment.Base RELATIVE POINTER;      status: Zone.Status;      [node, status] ¬ ResidentHeap.MakeNode[eTableSize];      IF status # okay THEN Bug[unableToAllocateResidentHeapNode];      newETable ¬ @ResidentHeap.first64K[node];      SetUpETable[newETable];      AddToOverflow[lvBucketInfo, newETable];      lvBucketInfo.bucketHandle.header.eTablesInOverflowCount ¬         SUCC[lvBucketInfo.bucketHandle.header.eTablesInOverflowCount];      DoBucketIO[lvBucketInfo, write]; -- gets the header changes out there      status ¬ ResidentHeap.FreeNode[node];      IF status # okay THEN Bug[unableToFreeResidentHeapNode];      END;    END;     DeleteETable: PUBLIC PROCEDURE [    token: VolTable.LVToken, fileID: File.ID] =    BEGIN    lvBucketInfo: ETable.LVBucketInfo ¬ ETableInternal.AcquireLVBucketInfo[token];    DeleteETableInternal[      lvBucketInfo, fileID ! ETable.Error =>         lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo]];    lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo];    END;       DeleteETableInternal: PUBLIC PROCEDURE [    lvBucketInfo: ETable.LVBucketInfo, fileID: File.ID] =    BEGIN    -- Deletes the etable representing the file fileID. If it's in the bucket,     -- the bucket will be compacted. If it's in the overflow, it will be    -- turned into a free etable entry and no compaction will be done.     -- The cache will be invalidated.    countToMove: CARDINAL;    countToSmashAway: CARDINAL;    FindETable[lvBucketInfo, fileID];    IF lvBucketInfo.eTableHandle = NIL THEN       ERROR ETable.Error[fileNotFound];      countToSmashAway ¬ GetETableSize[lvBucketInfo.eTableHandle];    IF lvBucketInfo.inBucket  THEN      BEGIN --it's in the bucket      -- we got it, so smash it      countToMove ¬ CARDINAL[	@lvBucketInfo.bucketHandle[lvBucketInfo.bucketHandle.header.firstFree] -	lvBucketInfo.eTableHandle] - countToSmashAway;      Inline.LongCOPY[	from: lvBucketInfo.eTableHandle + countToSmashAway, 	nwords: countToMove,	to: lvBucketInfo.eTableHandle];      lvBucketInfo.bucketHandle.header.firstFree ¬ 	lvBucketInfo.bucketHandle.header.firstFree - countToSmashAway;      DoBucketIO[lvBucketInfo, write]; -- does copy and then primary etablefile      lvBucketInfo.inBucket ¬ FALSE;      END    ELSE --it's in the overflow      BEGIN      -- make the etable appear free, but don't compact it.      lvBucketInfo.eTableHandle.header.fileID ¬ File.nullID;      lvBucketInfo.eTableHandle.header.length ¬ countToSmashAway;      DoOverflowIO[lvBucketInfo, write];      lvBucketInfo.bucketHandle.header.eTablesInOverflowCount ¬        PRED[lvBucketInfo.bucketHandle.header.eTablesInOverflowCount];      DoBucketIO[lvBucketInfo, write];      END;    lvBucketInfo.fileID ¬ File.nullID;    lvBucketInfo.eTableHandle ¬ NIL;    END;      DoBucketIO: PUBLIC PROCEDURE [    lvBucketInfo: ETable.LVBucketInfo, op: ETableInternal.Operation] =      BEGIN      -- If op = read, bring bucket from the primary file on disk to the one page       -- mapped buffer area represented by lvbucketInfo.bucketHandle.      -- If op = write, force write the one page bucket buffer to its       -- corresponding disk page on the copy and then the primary etable files.       channel: DiskBackingStore.ChannelHandle;      found: BOOLEAN;      interval: VM.Interval;      readOnly: BOOLEAN;      run: BackingStore.Run;      sv: VolTable.SVDesc; -- needed by VolTable      diskData: LONG POINTER TO DiskBackingStore.Data ¬	DiskBackingStore.PDiskDataFromPBSData[@run.data];      eTable: ETable.ETableHandle ¬ 	IF op = read THEN lvBucketInfo.primaryETableHandle	ELSE lvBucketInfo.copyETableHandle;      volumePage: Volume.PageNumber ¬ WhereIsFilePageOnVolume[	lvBucketInfo.currentBucketPage, eTable].volumePage;      IF volumePage = ETableInternal.nullETableFilePage THEN Bug[bucketNotFound];      [found, channel] ¬         VolTable.FindSV[lvBucketInfo.lvHandle.vID, volumePage, @sv];      IF ~found THEN Bug[noSuchSV];      readOnly ¬ VolTable.GetLVStatus[lvBucketInfo.lvHandle.vID].readOnly;      IF op = write AND readOnly THEN Bug[readOnlyVolume];      run.count ¬ ETableInternal.normalBucketSize;      WITH d: diskData SELECT FileLock.lockingEnabled FROM        TRUE => d.lock ¬ FileLock.nullLockHandle;	FALSE => d.file ¬	  ETableInternal.ConvertTypeToID[PilotFileTypesExtraExtras.tETable];	ENDCASE;      diskData.type ¬ PilotFileTypesExtraExtras.tETable;       diskData.channelHandle ¬ channel;      diskData.volumePage ¬ sv.pvPageOfSV + volumePage;      [diskData.filePageLow, diskData.filePageHigh] ¬	DiskBackingStore.UnpackFilePageNumber[volumePage];      diskData.fileAttributes ¬ [temporary: FALSE, readOnly: readOnly];      -- bucketHandle MUST be mapped      interval ¬ [        Environment.PageFromLongPointer[lvBucketInfo.bucketHandle], 	ETableInternal.normalBucketSize];      SELECT op FROM         read => VM.CopyIn[interval, DiskBackingStore.Transfer, run, wait];	write =>	  BEGIN	  VM.CopyOut[interval, DiskBackingStore.Transfer, run, wait];	  -- now write to primary (copy was just done)	  volumePage ¬ WhereIsFilePageOnVolume[	    lvBucketInfo.currentBucketPage,	    lvBucketInfo.primaryETableHandle].volumePage;	  IF volumePage = ETableInternal.nullETableFilePage THEN 	    Bug[bucketNotFound];	  diskData.volumePage ¬ sv.pvPageOfSV + volumePage;	  [diskData.filePageLow, diskData.filePageHigh] ¬	    DiskBackingStore.UnpackFilePageNumber[volumePage];	  VM.CopyOut[interval, DiskBackingStore.Transfer, run, wait];	  END;	ENDCASE;      END;        DoOverflowIO: PUBLIC PROCEDURE [    lvBucketInfo: ETable.LVBucketInfo, op: ETableInternal.Operation] =    BEGIN    -- If op = read, bring overflow block of the primary file from disk to        -- overflow mapped buffer area represented by lvbucketInfo.overflowHandle.    -- If op = write, force write the  buffer to its corresponding disk pages of    -- the copy etable file and then of the primary etable file.    channel: DiskBackingStore.ChannelHandle;    eTable: ETable.ETableHandle;    found: BOOLEAN;    maxNumberOfRuns: CARDINAL;    newRuns: ETable.PageGroupHandle;    node: Environment.Base RELATIVE POINTER;    readOnly: BOOLEAN;    status: Zone.Status;    sv: VolTable.SVDesc; -- needed by VolTable        GatherRuns: PROCEDURE [] =      BEGIN      allDone: BOOLEAN ¬ FALSE;      i, j: CARDINAL ¬ 0;      countLeft: CARDINAL ¬ maxNumberOfRuns;      page: File.PageNumber ¬ lvBucketInfo.currentOverflowPage;      startPage: File.PageNumber ¬ 0;      --  no need to null out newRuns entries, so just start stuffin'      UNTIL i = eTable.header.howManyGroups OR allDone DO	IF page < startPage + eTable.pageGroups[i].count THEN {	  offset: CARDINAL ¬ CARDINAL[page - startPage];	  newRuns[j].volumePage ¬ eTable.pageGroups[i].volumePage + offset;	  newRuns[j].count ¬ MIN[eTable.pageGroups[i].count - offset, countLeft];	  IF (countLeft ¬ (countLeft - CARDINAL[newRuns[j].count])) > 0 THEN  {	    page ¬ page + newRuns[j].count;	    j ¬ j + 1 }	  ELSE allDone ¬ TRUE };	startPage ¬ startPage + eTable.pageGroups[i].count;	i ¬ i + 1;	ENDLOOP;      IF ~allDone THEN Bug[overflowBlockNotFound];      -- we must have found the entire block      newRuns.LENGTH ¬ j + 1;       -- using the first run page of the overflow block should be sufficient      [found, channel] ¬ 	VolTable.FindSV[lvBucketInfo.lvHandle.vID, newRuns[0].volumePage, @sv];      IF ~found THEN Bug[noSuchSV];      END;        DoIOToEachRun: PROCEDURE [] =      BEGIN      totalRunCount: Environment.PageCount ¬ 0;      run: BackingStore.Run;      diskData: LONG POINTER TO DiskBackingStore.Data ¬        DiskBackingStore.PDiskDataFromPBSData[@run.data];      WITH d: diskData SELECT FileLock.lockingEnabled FROM        TRUE => d.lock ¬ FileLock.nullLockHandle;	FALSE => d.file ¬ 	  ETableInternal.ConvertTypeToID[PilotFileTypesExtraExtras.tETable];	ENDCASE;      diskData.type ¬ PilotFileTypesExtraExtras.tETable;       diskData.channelHandle ¬ channel;      diskData.fileAttributes ¬ [temporary: FALSE, readOnly: readOnly];      FOR i: CARDINAL IN [0..LENGTH[newRuns]) DO        run.count ¬ newRuns[i].count;        diskData.volumePage ¬	  sv.pvPageOfSV + newRuns[i].volumePage;	[diskData.filePageLow, diskData.filePageHigh] ¬	  DiskBackingStore.UnpackFilePageNumber[newRuns[i].volumePage];	IF op = read THEN 	  VM.CopyIn[[	    Environment.PageFromLongPointer[lvBucketInfo.overflowHandle] +	    totalRunCount, run.count], DiskBackingStore.Transfer, run, wait]	ELSE --write --	  VM.CopyOut[[	    Environment.PageFromLongPointer[lvBucketInfo.overflowHandle] +	    totalRunCount, run.count], DiskBackingStore.Transfer, run, wait];	totalRunCount ¬ totalRunCount + run.count;        ENDLOOP;      END;        readOnly ¬ VolTable.GetLVStatus[lvBucketInfo.lvHandle.vID].readOnly;    IF op = write AND readOnly THEN Bug[readOnlyVolume];    eTable ¬       IF op = read THEN lvBucketInfo.primaryETableHandle      ELSE lvBucketInfo.copyETableHandle;    -- maximum number of runs will equal the size of the overflow area    maxNumberOfRuns ¬ ETableInternal.overflowBufferSize;    [node, status] ¬      ResidentHeap.MakeNode[CARDINAL[SIZE[ETable.PageGroup]*maxNumberOfRuns]];    IF status # okay THEN Bug[unableToAllocateResidentHeapNode];    newRuns ¬ DESCRIPTOR[@ResidentHeap.first64K[node], maxNumberOfRuns];    GatherRuns[];     DoIOToEachRun[];     IF op = write THEN {      -- we've done the copy, now to the primary.      eTable ¬ lvBucketInfo.primaryETableHandle;      GatherRuns[];      DoIOToEachRun[] };    status ¬ ResidentHeap.FreeNode[node];    IF status # okay THEN Bug[unableToFreeResidentHeapNode];    END;    FindETable: PUBLIC PROCEDURE [    lvBucketInfo: ETable.LVBucketInfo, fileID: File.ID] =    BEGIN    -- Locates fileID's etable by first trying the bucket, then the overflow.     currentETable: ETable.ETableHandle;    newBucketPage: File.PageNumber;    stopHere: LONG POINTER;    IF fileID = lvBucketInfo.fileID THEN idHit ¬ idHit.SUCC;  -- it's still cached (we hope)    -- okay, so it wasn't found, let's go hunting    -- are we at least in the right bucket?    newBucketPage ¬ lvBucketInfo.firstBucketPage + Hash[lvBucketInfo, fileID];    IF newBucketPage # lvBucketInfo.currentBucketPage THEN {      lvBucketInfo.currentBucketPage ¬ newBucketPage;      DoBucketIO[lvBucketInfo, read]};    -- okay, now we got the right bucket     lvBucketInfo.fileID ¬ File.nullID;    lvBucketInfo.eTableHandle ¬ NIL;    lvBucketInfo.inBucket ¬ FALSE;    -- start searching for the file's etable     currentETable ¬ @lvBucketInfo.bucketHandle.eTables[0];    stopHere ¬ @lvBucketInfo.bucketHandle[lvBucketInfo.bucketHandle.header.firstFree];    UNTIL (currentETable = stopHere) OR (lvBucketInfo.eTableHandle # NIL) DO      IF currentETable.header.fileID = fileID THEN {  	lvBucketInfo.fileID ¬ fileID;	lvBucketInfo.eTableHandle ¬ currentETable;	lvBucketInfo.inBucket ¬ TRUE}      ELSE currentETable ¬ currentETable + GetETableSize[currentETable];      ENDLOOP;    IF (lvBucketInfo.eTableHandle = NIL) AND        (lvBucketInfo.bucketHandle.header.eTablesInOverflowCount > 0) THEN        SearchOverflow[lvBucketInfo, fileID]    END;      GetCurrentETFileSize: PUBLIC -- ETableInternal -- PROCEDURE [    lvBucketInfo: ETable.LVBucketInfo] RETURNS [size: File.PageCount] =    BEGIN    size ¬       ETableInternal.fileHeaderSize + (2 * lvBucketInfo.fileHandle.maxETableSize) +       lvBucketInfo.fileHandle.preImageSize + lvBucketInfo.fileHandle.numberOfBuckets +      lvBucketInfo.fileHandle.overflowSize     -- bucketSize is one, so no need to multiply this to numberOfBuckets    END;      GetETableSize: PROCEDURE [eTable: ETable.ETableHandle] RETURNS [CARDINAL] = INLINE {    RETURN[CARDINAL[      SIZE[ETable.ETableHeader] +       SIZE[ETable.PageGroup] * eTable.header.howManyGroups]] };      GetFileAttributes: PUBLIC PROCEDURE [token: VolTable.LVToken, fileID: File.ID]    RETURNS [       fileSize: File.PageCount, temporary: BOOLEAN, bootable: BOOLEAN,       type: File.Type, howManyGroups: CARDINAL] =    BEGIN    lvBucketInfo: ETable.LVBucketInfo ¬ ETableInternal.AcquireLVBucketInfo[token];    [fileSize, temporary, bootable, type, howManyGroups] ¬       GetFileAttributesInternal[lvBucketInfo, fileID !         ETable.Error => 	   lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo]];    lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo];    END;    GetFileAttributesInternal: PROCEDURE [    lvBucketInfo: ETable.LVBucketInfo, fileID: File.ID]    RETURNS [      fileSize: File.PageCount, temporary: BOOLEAN, bootable: BOOLEAN,       type: File.Type, howManyGroups: CARDINAL] =    BEGIN    FindETable[lvBucketInfo, fileID];    IF lvBucketInfo.eTableHandle = NIL THEN ERROR ETable.Error[fileNotFound];    -- must have found the file's eTable    temporary ¬ lvBucketInfo.eTableHandle.header.temporary;    bootable ¬ lvBucketInfo.eTableHandle.header.bootable;    type ¬ lvBucketInfo.eTableHandle.header.type;    howManyGroups ¬ lvBucketInfo.eTableHandle.header.howManyGroups;    fileSize ¬ GetSize[lvBucketInfo.eTableHandle];    END;      GetPageGroup: PUBLIC PROCEDURE [    token: VolTable.LVToken, fileID: File.ID, filePage: File.PageNumber]    RETURNS [found: BOOLEAN ¬ FALSE, group: KernelFile.PageGroup] =    BEGIN    -- Returns the group containing fileID's filePage by searching through the     -- etable's page groups of fileID.    lvBucketInfo: ETable.LVBucketInfo ¬      ETableInternal.AcquireLVBucketInfo[token];    [found, group] ¬      GetPageGroupInternal[lvBucketInfo, token, fileID, filePage !        ETable.Error =>	  lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo]];    lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo];    END;      GetPageGroupInternal: PUBLIC PROCEDURE [    lvBucketInfo: ETable.LVBucketInfo, token: VolTable.LVToken,    fileID: File.ID, filePage: File.PageNumber]    RETURNS [found: BOOLEAN ¬ FALSE, group: KernelFile.PageGroup] =     BEGIN    -- Returns the group containing fileID's filePage by searching through the     -- etable's page groups of fileID.    pageGroup: ETable.PageGroup;    volumePage: Volume.PageNumber;    FindETable[lvBucketInfo, fileID];    IF lvBucketInfo.eTableHandle # NIL THEN       BEGIN      [volumePage, pageGroup] ¬ 	WhereIsFilePageOnVolume[filePage, lvBucketInfo.eTableHandle];      IF volumePage # ETableInternal.nullETableFilePage THEN {	group.filePage ¬ filePage - (volumePage - pageGroup.volumePage);	group.volumePage ¬ pageGroup.volumePage;	group.nextFilePage ¬ group.filePage + pageGroup.count ;	found ¬ TRUE}      END;     END;   GetSize: PUBLIC -- ETableInternal -- PROCEDURE [eTable: ETable.ETableHandle]    RETURNS [fileSize: File.PageCount] =    -- returns file size described by the etable    BEGIN    howManyGroups: CARDINAL ¬ eTable.header.howManyGroups;    fileSize ¬ 0;    FOR i: CARDINAL IN [0..howManyGroups) DO      fileSize ¬ fileSize + eTable.pageGroups[i].count;      ENDLOOP;    END;    Hash: PUBLIC PROCEDURE [lvBucketInfo: ETable.LVBucketInfo, fileID: File.ID]    RETURNS [bucket: CARDINAL] =    BEGIN    -- we would really like the hash to be number of buckets minus 1 (never 0) AND    -- the number of buckets be a power of 2. Then we could do the following:    -- RETURN [Inline.DBITAND[lvBucketInfo.hash, LOOPHOLE[fileID, LONG UNSPECIFIED]]];    -- Should save a few more cycles than the MOD.    -- Doing a short MOD is cheaper than a long mod.    RETURN[CARDINAL[LOOPHOLE[fileID, LONG UNSPECIFIED] MOD lvBucketInfo.hash]];    END;      InsertPageGroup: PUBLIC PROCEDURE [    token: VolTable.LVToken, fileID: File.ID,     groupPtr: LONG POINTER TO KernelFile.PageGroup] =    BEGIN    -- Inserts the page group, described by groupPtr, at the end of the other    -- page groups for that etable, combining it with the last page group if possible.    -- Breaks the group given into two page groups, if the page group count exceeds    -- a cardinal value.    -- The cache will be invalidated, if the etable moves from the bucket to the     -- overflow or is already in the overflow.    additionalPageGroups: CARDINAL;    groupPtrCount: LONG CARDINAL;    lastGroup: ETable.PageGroupPtr;    newETable: ETable.ETableHandle;    node: Environment.Base RELATIVE POINTER;    newPageGroupIndex: CARDINAL;    status: Zone.Status;        lvBucketInfo: ETable.LVBucketInfo ¬ ETableInternal.AcquireLVBucketInfo[token];    FindETable[lvBucketInfo, fileID];    IF lvBucketInfo.eTableHandle = NIL THEN {      lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo];      ERROR ETable.Error[fileNotFound] };    newPageGroupIndex ¬ lvBucketInfo.eTableHandle.header.howManyGroups;     lastGroup ¬ @lvBucketInfo.eTableHandle.pageGroups[newPageGroupIndex-1];    groupPtrCount ¬ groupPtr.nextFilePage - groupPtr.filePage;    SELECT TRUE FROM      (groupPtr.volumePage = lastGroup.volumePage + lastGroup.count) AND      (lastGroup.count + groupPtrCount <= LAST[CARDINAL]) =>	-- combine new page group with the last one	BEGIN	lastGroup.count ¬ CARDINAL[lastGroup.count + groupPtrCount];	IF lvBucketInfo.inBucket THEN DoBucketIO[lvBucketInfo, write]	ELSE DoOverflowIO[lvBucketInfo, write];	lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo];	RETURN;	END;      (groupPtrCount > LAST[CARDINAL]) =>        -- break the group given into n-many new page groups and insert them one by one.	BEGIN	countProcessed: LONG CARDINAL ¬ 0;	thisPageGroupCount: CARDINAL;	overlap: LONG CARDINAL ¬ groupPtrCount MOD LAST[CARDINAL];	additionalPageGroups ¬ 	  CARDINAL[IF overlap = 0 THEN groupPtrCount/LAST[CARDINAL]	  ELSE groupPtrCount/LAST[CARDINAL] + 1];	[newETable, node] ¬ MakeNewETable[	  lvBucketInfo, additionalPageGroups, newPageGroupIndex];	UNTIL countProcessed = groupPtrCount DO 	  thisPageGroupCount ¬	    CARDINAL[MIN[LONG[LAST[CARDINAL]], groupPtrCount - countProcessed]];	  newETable.pageGroups[newPageGroupIndex] ¬ [	    count: thisPageGroupCount, volumePage: groupPtr.volumePage + countProcessed];	  newPageGroupIndex ¬ newPageGroupIndex + 1;	  countProcessed ¬ countProcessed + thisPageGroupCount;	  ENDLOOP;	END;      ENDCASE => 	BEGIN -- make a new etable with the one inserted page group	additionalPageGroups ¬ 1;	[newETable, node] ¬ MakeNewETable[	  lvBucketInfo, additionalPageGroups, newPageGroupIndex];	newETable.pageGroups[newPageGroupIndex] ¬ [	  count: CARDINAL[groupPtrCount], volumePage: groupPtr.volumePage];	END;    -- we have inserted one or two new page groups in a new etable    ReplaceOldETableInternal[lvBucketInfo, newETable !      ETable.Error, Volume.InsufficientSpace =>        lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo]];    status ¬ ResidentHeap.FreeNode[node];    IF status # okay THEN Bug[unableToFreeResidentHeapNode];    lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo];    END;      MakeNewETable: PROCEDURE [    lvBucketInfo: ETable.LVBucketInfo, additionalPageGroups: CARDINAL,     newIndex: CARDINAL]     RETURNS [newETable: ETable.ETableHandle, node: Environment.Base RELATIVE POINTER] =    BEGIN    additionalSize: CARDINAL;    nwords: CARDINAL;    status: Zone.Status;    oldETableSize: CARDINAL ¬ GetETableSize[lvBucketInfo.eTableHandle];    [node, status] ¬ ResidentHeap.MakeNode[ -- make a node of the new etable size --      CARDINAL[oldETableSize + additionalPageGroups * SIZE[ETable.PageGroup]]];    IF status # okay THEN Bug[unableToAllocateResidentHeapNode];    newETable ¬ @ResidentHeap.first64K[node];    additionalSize ¬ additionalPageGroups * SIZE[ETable.PageGroup];    nwords ¬ SIZE[ETable.ETableHeader] + newIndex * SIZE[ETable.PageGroup];    -- leave space for page groups that require changing    Inline.LongCOPY[      from: lvBucketInfo.eTableHandle, nwords: nwords, to: newETable];    Inline.LongCOPY[      from: lvBucketInfo.eTableHandle + nwords,      nwords: oldETableSize - nwords,       to: newETable + nwords + additionalSize];    newETable.header.howManyGroups ¬       lvBucketInfo.eTableHandle.header.howManyGroups + additionalPageGroups;    END;    ReplaceOldETableInternal: PUBLIC --ETableInternal-- PROCEDURE [    lvBucketInfo: ETable.LVBucketInfo, newETable: ETable.ETableHandle] =    BEGIN        -- Replaces the etable of lvBucketInfo.eTableHandle with newETable.        oldFreeSpace: CARDINAL;    offsetInOverflow: LONG CARDINAL;    oldETable: ETable.ETableHandle ¬ lvBucketInfo.eTableHandle;    oldOverflowPage: File.PageNumber ¬ lvBucketInfo.currentOverflowPage;    newETableSize: CARDINAL ¬ GetETableSize[newETable];    oldETableSize: CARDINAL ¬ GetETableSize[lvBucketInfo.eTableHandle];    nextETableHandle: ETable.ETableHandle ¬ LOOPHOLE[       lvBucketInfo.eTableHandle + oldETableSize];    fitsInBucket: BOOLEAN ¬     -- Only meaningful if lvBucketInfo.inBucket = TRUE      ((lvBucketInfo.bucketHandle.header.firstFree + newETableSize - oldETableSize) <=      maxEntryInBucket AND      newETable.header.howManyGroups <= ETableInternal.maxPageGroupSize);          IF (lvBucketInfo.inBucket AND fitsInBucket) THEN           -- It's in the bucket, and will still fit, so replace old with new.	       BEGIN        Inline.LongCOPY[	  from: lvBucketInfo.eTableHandle + oldETableSize,	  nwords: CARDINAL[@lvBucketInfo.bucketHandle[                  lvBucketInfo.bucketHandle.header.firstFree] 		  - lvBucketInfo.eTableHandle] - oldETableSize,	  to: lvBucketInfo.eTableHandle];       lvBucketInfo.eTableHandle ¬ LOOPHOLE[	  @lvBucketInfo.bucketHandle[lvBucketInfo.bucketHandle.header.firstFree] - 	  oldETableSize];       Inline.LongCOPY[	  from: newETable, 	  nwords: newETableSize, 	  to: lvBucketInfo.eTableHandle];       lvBucketInfo.bucketHandle.header.firstFree ¬	  lvBucketInfo.bucketHandle.header.firstFree - oldETableSize + newETableSize;       DoBucketIO[lvBucketInfo, write];       RETURN;       END;          IF (lvBucketInfo.inBucket AND ~fitsInBucket) THEN             -- It's in the bucket, but no longer will fit, so move it to the overflow.	       BEGIN       AddToOverflow[lvBucketInfo, newETable];        lvBucketInfo.eTableHandle ¬ oldETable;                    lvBucketInfo.bucketHandle.header.eTablesInOverflowCount ¬           SUCC[lvBucketInfo.bucketHandle.header.eTablesInOverflowCount];       -- DeleteETableInternal will find the old etable in the bucket,        -- smash it, forceout the bucket, including the header,        -- and eliminate our cache of the overflow.               DeleteETableInternal[lvBucketInfo, lvBucketInfo.fileID];       RETURN;       END;	       -- Its in the overflow           offsetInOverflow ¬        (lvBucketInfo.currentOverflowPage - lvBucketInfo.firstOverflowPage) *        Environment.wordsPerPage +        (LOOPHOLE[lvBucketInfo.eTableHandle, LONG CARDINAL] -       LOOPHOLE[lvBucketInfo.overflowHandle, LONG CARDINAL]);    IF ((newETableSize + lvBucketInfo.bucketHandle.header.firstFree) <= 	   maxEntryInBucket) AND          (newETable.header.howManyGroups <= ETableInternal.maxPageGroupSize) THEN       		       -- CASE 1: Although the old ETable is in the overflow, the new         --         ETable fits into its appropriate bucket.        BEGIN        lvBucketInfo.eTableHandle ¬ LOOPHOLE[	  @lvBucketInfo.bucketHandle[lvBucketInfo.bucketHandle.header.firstFree]];       Inline.LongCOPY[	  from: newETable, nwords: newETableSize, to: lvBucketInfo.eTableHandle];       lvBucketInfo.bucketHandle.header.firstFree ¬	  lvBucketInfo.bucketHandle.header.firstFree + newETableSize;       lvBucketInfo.bucketHandle.header.eTablesInOverflowCount ¬ 	  lvBucketInfo.bucketHandle.header.eTablesInOverflowCount - 1;       DoBucketIO[lvBucketInfo, write];              -- Now write null to old etable and flush the cache.              oldETable.header.fileID ¬ File.nullID;       oldETable.header.length ¬ oldETableSize;       DoOverflowIO[lvBucketInfo, write];       lvBucketInfo.inBucket ¬ TRUE;       RETURN;       END;        IF (offsetInOverflow + oldETableSize =       LOOPHOLE[lvBucketInfo.freeSpaceInOverflow, LONG CARDINAL] AND        CARDINAL[LOOPHOLE[lvBucketInfo.eTableHandle, LONG CARDINAL] -       LOOPHOLE[lvBucketInfo.overflowHandle, LONG CARDINAL] +       newETableSize + SIZE[ETable.ETableHeader]] <= blockSize ) THEN        	       -- CASE 2: The new ETable will not fit into its appropriate bucket.          --         However, this the last ETable in the overflow and there        --         is room in the overflow block for the new ETable.           BEGIN             Inline.LongCOPY[	  from: newETable, nwords: newETableSize, to: lvBucketInfo.eTableHandle];       lvBucketInfo.freeSpaceInOverflow ¬ 	  lvBucketInfo.freeSpaceInOverflow - oldETableSize + newETableSize;       IF lvBucketInfo.currentOverflowPage ~= lvBucketInfo.firstOverflowPage THEN           BEGIN	  DoOverflowIO[lvBucketInfo, write];	  lvBucketInfo.currentOverflowPage ¬ lvBucketInfo.firstOverflowPage;          DoOverflowIO[lvBucketInfo, read];	  lvBucketInfo.fileID ¬ File.nullID;          lvBucketInfo.eTableHandle ¬ NIL; 	  END;       lvBucketInfo.overflowHandle.header.freeSpace ¬ 	  lvBucketInfo.freeSpaceInOverflow;       DoOverflowIO[lvBucketInfo, write];       RETURN;       END;	       IF nextETableHandle.header.fileID = File.nullID AND        nextETableHandle.header.length >=           (newETableSize - oldETableSize + SIZE[ETable.ETableHeader]) AND        (LOOPHOLE[oldETable, LONG CARDINAL] -           LOOPHOLE[lvBucketInfo.overflowHandle, LONG CARDINAL])/	  Environment.wordsPerPage =           (LOOPHOLE[oldETable, LONG CARDINAL] + newETableSize + 	  SIZE[ETable.ETableHeader] -	  LOOPHOLE[lvBucketInfo.overflowHandle, LONG CARDINAL])/blockSize THEN               -- CASE 3: The new ETable will not fit into its appropriate bucket.              --         Although the old ETable is not the last in the overflow, there       --         is a sufficiently large freed area after it (and within the         --         same page) to accommodate the new ETable.              BEGIN             oldFreeSpace ¬ nextETableHandle.header.length;       Inline.LongCOPY[	  from: newETable, nwords: newETableSize, to: oldETable];       nextETableHandle ¬ nextETableHandle - oldETableSize + newETableSize;       nextETableHandle.header.fileID ¬ File.nullID;       nextETableHandle.header.length ¬           oldFreeSpace - (newETableSize - oldETableSize);       DoOverflowIO[lvBucketInfo, write];       RETURN;       END;	      	   	       -- CASE 4: The new ETable will neither fit into its appropriate bucket         --         nor can it overlay the old ETable.  Put the new ETable at the       --         end of the overflow and then delete the old one.	       AddToOverflow[lvBucketInfo, newETable];               -- Now write null to old etable and flush the cache.               IF lvBucketInfo.currentOverflowPage # oldOverflowPage THEN {          lvBucketInfo.currentOverflowPage ¬ oldOverflowPage;          DoOverflowIO[lvBucketInfo, read]};       oldETable.header.fileID ¬ File.nullID;       oldETable.header.length ¬ oldETableSize;       DoOverflowIO[lvBucketInfo, write];       lvBucketInfo.fileID ¬ File.nullID;       lvBucketInfo.eTableHandle ¬ NIL;      END;      ReplaceOldETable: PUBLIC --ETableInternal-- PROCEDURE [     token: VolTable.LVToken, newETable: ETable.ETableHandle] =     BEGIN     lvBucketInfo: ETable.LVBucketInfo ¬         ETableInternal.AcquireLVBucketInfo[token];     FindETable[lvBucketInfo, newETable.header.fileID];     IF lvBucketInfo.eTableHandle = NIL THEN ERROR ETable.Error[fileNotFound];     ReplaceOldETableInternal[lvBucketInfo, newETable !        ETable.Error, Volume.InsufficientSpace =>           lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo]];     lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo];     END;        ReplacePage: PUBLIC PROCEDURE [    token: VolTable.LVToken, fileID: File.ID, oldFilePage: File.PageNumber,    newVolumePage: Volume.PageNumber] =    BEGIN    -- Modifies the etable of fileID to make oldFilePage's volumePage be newVolumePage.    -- Zero, one, or two additional page groups will be added to the etable.    -- The cache will be invalidated, if the etable moves from the bucket to the     -- overflow or is already in the overflow.    lvBucketInfo: ETable.LVBucketInfo ¬ ETableInternal.AcquireLVBucketInfo[token];    ReplacePageInternal[lvBucketInfo, fileID, oldFilePage, newVolumePage !      ETable.Error, Volume.InsufficientSpace =>        lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo]];    lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo];    END;      ReplacePageInternal: PUBLIC --ETableInternal -- PROCEDURE [    lvBucketInfo: ETable.LVBucketInfo, fileID: File.ID, oldFilePage: File.PageNumber,    newVolumePage: Volume.PageNumber] =    BEGIN    additionalPageGroups: CARDINAL;    currentFilePage: File.PageNumber ¬ 0;             found: BOOLEAN ¬ FALSE;    i: CARDINAL ¬ 0;    newETable: ETable.ETableHandle;    node: Environment.Base RELATIVE POINTER;    status: Zone.Status;        FindETable[lvBucketInfo, fileID];    IF lvBucketInfo.eTableHandle = NIL THEN ERROR ETable.Error[fileNotFound];    UNTIL (i = lvBucketInfo.eTableHandle.header.howManyGroups) OR found DO       SELECT TRUE FROM	((oldFilePage = currentFilePage) AND	  (lvBucketInfo.eTableHandle.pageGroups[i].count = 1)) =>	  BEGIN	  -- this is a special case, since no new page groups have to be added	  lvBucketInfo.eTableHandle.pageGroups[i].volumePage ¬ newVolumePage;	  IF lvBucketInfo.inBucket THEN DoBucketIO[lvBucketInfo, write]	  ELSE DoOverflowIO[lvBucketInfo, write];	  RETURN;	  END;	(oldFilePage = currentFilePage) => 	  BEGIN	  -- insert one page group before this page group	  additionalPageGroups ¬ 1;	  [newETable, node] ¬ MakeNewETable[lvBucketInfo, additionalPageGroups, i];	  newETable.pageGroups[i] ¬ [newVolumePage, 1];	  newETable.pageGroups[i+1] ¬ [ 	    SUCC[lvBucketInfo.eTableHandle.pageGroups[i].volumePage],	    PRED[lvBucketInfo.eTableHandle.pageGroups[i].count]];	  found ¬ TRUE;	  END;	(oldFilePage = 	  (currentFilePage + lvBucketInfo.eTableHandle.pageGroups[i].count - 1)) =>	  BEGIN	  -- insert one page group after this page group	  additionalPageGroups ¬ 1;	  [newETable, node] ¬ MakeNewETable[lvBucketInfo, additionalPageGroups, i + 1];	  newETable.pageGroups[i] ¬ [	    lvBucketInfo.eTableHandle.pageGroups[i].volumePage, 	    PRED[lvBucketInfo.eTableHandle.pageGroups[i].count]];	  newETable.pageGroups[i+1] ¬ [newVolumePage, 1];	  found ¬ TRUE;	  END;	((oldFilePage > currentFilePage) AND (oldFilePage < 	  currentFilePage + lvBucketInfo.eTableHandle.pageGroups[i].count)) =>	  BEGIN	  -- insert one page group inbetween, causing two additional groups	  firstGroupSize: CARDINAL ¬ CARDINAL[oldFilePage - currentFilePage];	  additionalPageGroups ¬ 2;	  [newETable, node] ¬ MakeNewETable[lvBucketInfo, additionalPageGroups, i + 1];	  newETable.pageGroups[i] ¬ 	    [lvBucketInfo.eTableHandle.pageGroups[i].volumePage, firstGroupSize];	  newETable.pageGroups[i+1] ¬ [newVolumePage, 1];	  newETable.pageGroups[i+2] ¬ 	    [lvBucketInfo.eTableHandle.pageGroups[i].volumePage + firstGroupSize + 1, 	    lvBucketInfo.eTableHandle.pageGroups[i].count - firstGroupSize - 1];	  found ¬ TRUE;	  END;	ENDCASE => NULL;      currentFilePage ¬         currentFilePage + lvBucketInfo.eTableHandle.pageGroups[i].count;      i ¬ i + 1;         ENDLOOP;    IF ~found THEN ERROR ETable.Error[pageGroupNotFound];    ReplaceOldETableInternal[lvBucketInfo, newETable !      ETable.Error, Volume.InsufficientSpace =>         status ¬ ResidentHeap.FreeNode[node]];    status ¬ ResidentHeap.FreeNode[node];    IF status # okay THEN Bug[unableToFreeResidentHeapNode];    END;          SearchOverflow: PROCEDURE [    lvBucketInfo: ETable.LVBucketInfo, fileID: File.ID] =    BEGIN    -- Searches for fileID's eTable in the overflow, block by block.    -- Stops when it has seen etableCountInOverflow of the bucket's header,    -- has exhausted the overflow (for future clients?) or is found.         SearchBlockForFile: PROCEDURE [      currentETable: ETable.ETableHandle, stopHere: LONG POINTER] =      BEGIN      UNTIL (currentETable = stopHere) OR (eTableCntInOverflow = 0) DO	SELECT TRUE FROM  	  currentETable.header.fileID = fileID => {	    lvBucketInfo.fileID ¬ fileID;	    lvBucketInfo.eTableHandle ¬ currentETable;	    EXIT };	  currentETable.header.fileID = File.nullID => 	    currentETable ¬ currentETable + currentETable.header.length;	  ENDCASE => {	    -- update how many etables hashed to the bucket in the overflow we've seen	    IF (Hash[lvBucketInfo, currentETable.header.fileID] = hashValue) THEN 	      eTableCntInOverflow ¬ PRED[eTableCntInOverflow];	    currentETable ¬ currentETable + GetETableSize[currentETable]}; -- and move on	ENDLOOP;      END;        alreadyLookedAt: LONG CARDINAL ¬ 0;    eTableCntInOverflow: CARDINAL;    eTableToStartWith: ETable.ETableHandle;    exhaustedOverflow: BOOLEAN ¬ FALSE;     firstFree: LONG CARDINAL;    hashValue: CARDINAL ¬ Hash[lvBucketInfo, fileID];    stopHere: LONG POINTER;    lvBucketInfo.eTableHandle ¬ NIL; -- just being paranoid    -- Let's make sure the first block is really read in before setting loop variables    IF lvBucketInfo.currentOverflowPage # lvBucketInfo.firstOverflowPage THEN {      lvBucketInfo.currentOverflowPage ¬ lvBucketInfo.firstOverflowPage;      DoOverflowIO[lvBucketInfo, read] };    firstFree ¬ LOOPHOLE[lvBucketInfo.overflowHandle.header.freeSpace];    eTableToStartWith ¬ @lvBucketInfo.overflowHandle.eTables[0]; -- only on first block    stopHere ¬ lvBucketInfo.overflowHandle + blockSize;    eTableCntInOverflow ¬ lvBucketInfo.bucketHandle.header.eTablesInOverflowCount;    -- the loop is set up in such way to avoid an extra read of the first block    -- or an extra block read at the tail end...note the exit within the loop...      DO       -- first block was read in already, just set it up for a search      alreadyLookedAt ¬ alreadyLookedAt + ETableInternal.overflowBufferSize;      IF (alreadyLookedAt * Environment.wordsPerPage > firstFree) THEN {        stopHere ¬ lvBucketInfo.overflowHandle + (firstFree MOD blockSize);	exhaustedOverflow ¬ TRUE };      SearchBlockForFile[eTableToStartWith, stopHere];       IF (lvBucketInfo.eTableHandle # NIL) OR (eTableCntInOverflow = 0) OR         (exhaustedOverflow) THEN EXIT;      eTableToStartWith ¬ LOOPHOLE[lvBucketInfo.overflowHandle]; --for non-first block      lvBucketInfo.currentOverflowPage ¬         lvBucketInfo.currentOverflowPage + ETableInternal.overflowBufferSize;      DoOverflowIO[lvBucketInfo, read];      ENDLOOP;    END;      SetBootable: PUBLIC PROCEDURE [    token: VolTable.LVToken, fileID: File.ID, bootable: BOOLEAN] =    BEGIN    -- sets the bootable bit in the etable header    lvBucketInfo: ETable.LVBucketInfo ¬ ETableInternal.AcquireLVBucketInfo[token];    FindETable[lvBucketInfo, fileID];    IF lvBucketInfo.eTableHandle = NIL THEN {      lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo];      ERROR ETable.Error[fileNotFound]};    -- must have found the file's eTable    lvBucketInfo.eTableHandle.header.bootable ¬ bootable;    IF lvBucketInfo.inBucket THEN DoBucketIO[lvBucketInfo, write]     ELSE DoOverflowIO[lvBucketInfo, write];    lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo];    END;      SetTemporary: PUBLIC PROCEDURE [    token: VolTable.LVToken, fileID: File.ID, temporary: BOOLEAN] =    BEGIN         -- Sets the temporary bit in the etable header        lvBucketInfo: ETable.LVBucketInfo ¬ ETableInternal.AcquireLVBucketInfo[token];    SetTemporaryInternal[lvBucketInfo, token, fileID, temporary !       ETable.Error => lvBucketInfo ¬        ETableInternal.ReleaseLVBucketInfo[lvBucketInfo]];    lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo];    END;      SetTemporaryInternal: PUBLIC PROCEDURE [    lvBucketInfo: ETable.LVBucketInfo, token: VolTable.LVToken,    fileID: File.ID, temporary: BOOLEAN] =    BEGIN        -- Sets the temporary bit in the etable header        FindETable[lvBucketInfo, fileID];    IF lvBucketInfo.eTableHandle = NIL THEN       ERROR ETable.Error[fileNotFound];          -- Must have found the file's eTable        lvBucketInfo.eTableHandle.header.temporary ¬ temporary;    IF lvBucketInfo.inBucket THEN DoBucketIO[lvBucketInfo, write]    ELSE DoOverflowIO[lvBucketInfo, write];    END;  ShrinkLastPageGroup: PUBLIC PROCEDURE [    token: VolTable.LVToken, fileID: File.ID,     groupPtr: LONG POINTER TO KernelFile.PageGroup] =    BEGIN    -- Shrinks the last page group of the file in the etable. If the number to shrink    -- is greater or equal to the last page group count, the entire group is deleted.    -- The semantics of groupPtr (KernelFile.PageGroup) have been changed for how it    -- is given and for how it is returned.     -- groupPtr is given in the form     --   [new size of file, nullVolumePage, current size of file]    -- groupPtr is returned in the form     -- [first deleted file page, volume page of first deleted, old size of file]     countLeftInGroup: CARDINAL;    countToShrink: CARDINAL;    lastGroup: ETable.PageGroup;    oldETableSize: CARDINAL;    oldPageGroupCount: CARDINAL;    lvBucketInfo: ETable.LVBucketInfo ¬ ETableInternal.AcquireLVBucketInfo[token];        FindETable[lvBucketInfo, fileID];    IF lvBucketInfo.eTableHandle = NIL THEN {      lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo];      ERROR ETable.Error[fileNotFound] };    oldETableSize ¬ GetETableSize[lvBucketInfo.eTableHandle];    oldPageGroupCount ¬ lvBucketInfo.eTableHandle.header.howManyGroups;    -- find me last page    lastGroup ¬ WhereIsFilePageOnVolume[      PRED[groupPtr.nextFilePage], lvBucketInfo.eTableHandle].containingPageGroup;    IF lastGroup = ETable.nullPageGroup THEN {      lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo];      ERROR ETable.Error[pageGroupNotFound] };    countToShrink ¬ CARDINAL[MIN[      LONG[lastGroup.count], (groupPtr.nextFilePage - groupPtr.filePage)]];      -- this avoids overflowing groupPtr.nextFilePage - groupPtr.filePage.    countLeftInGroup ¬ lastGroup.count - countToShrink;    SELECT TRUE FROM      (countToShrink < lastGroup.count) => 	BEGIN -- update page group entry	lvBucketInfo.eTableHandle.pageGroups[oldPageGroupCount-1].count ¬ 	  countLeftInGroup;	IF lvBucketInfo.inBucket THEN	  DoBucketIO[lvBucketInfo, write]	ELSE DoOverflowIO[lvBucketInfo, write];	END;      ENDCASE => -- its equal so need to get rid of entire page group -- 	BEGIN -- only etable header remains iff one page group per this file	lvBucketInfo.eTableHandle.header.howManyGroups ¬ PRED[oldPageGroupCount];	IF lvBucketInfo.eTableHandle.header.howManyGroups = 0 THEN 	  Bug[noPageGroupsRemaining]; -- more paranoia -- 	-- determine if it is in bucket or overflow	IF lvBucketInfo.inBucket THEN {	  -- in bucket 	  Inline.LongCOPY[	    from: lvBucketInfo.eTableHandle + oldETableSize,	    nwords: CARDINAL[	      @lvBucketInfo.bucketHandle[		lvBucketInfo.bucketHandle.header.firstFree] -		(lvBucketInfo.eTableHandle + oldETableSize)],	    to: lvBucketInfo.eTableHandle + oldETableSize - 	      SIZE[ETable.PageGroup]];	  lvBucketInfo.bucketHandle.header.firstFree ¬	    lvBucketInfo.bucketHandle.header.firstFree - SIZE[ETable.PageGroup];	  DoBucketIO[lvBucketInfo, write] }	ELSE { -- in overflow --	  freedPageGroup: ETable.ETableHandle ¬ LOOPHOLE[	    @lvBucketInfo.eTableHandle.pageGroups[oldPageGroupCount-1]];	  freedPageGroup.header.fileID ¬ File.nullID;	  freedPageGroup.header.length ¬ SIZE[ETable.PageGroup];	  DoOverflowIO[lvBucketInfo, write] }; 	END;    -- fix up groupPtr for the client leaving groupPtr.nextFilePage the same    groupPtr.filePage ¬ groupPtr.nextFilePage - countToShrink;    groupPtr.volumePage ¬ lastGroup.volumePage + countLeftInGroup;    lvBucketInfo ¬ ETableInternal.ReleaseLVBucketInfo[lvBucketInfo];    END;      WhereIsFilePageOnVolume: PUBLIC -- ETableInternal -- PROCEDURE [    page: File.PageNumber, eTable: ETable.ETableHandle]     RETURNS [      volumePage: Volume.PageNumber ¬ ETableInternal.nullETableFilePage,      containingPageGroup: ETable.PageGroup ¬ ETable.nullPageGroup,      index: CARDINAL] =    BEGIN    howManyGroups: CARDINAL ¬ eTable.header.howManyGroups;    startPage: File.PageNumber ¬ 0;    FOR index IN [0..howManyGroups) DO      IF page < startPage + eTable.pageGroups[index].count THEN {	volumePage ¬ 	  eTable.pageGroups[index].volumePage + (page - startPage);	containingPageGroup ¬ eTable.pageGroups[index];	RETURN }      ELSE startPage ¬ startPage + eTable.pageGroups[index].count;      ENDLOOP;    END;            END.  LOG  16-Jun-86 18:25:19	ET	Created file.20-Jun-86 15:03:13	ET	Twiddled MakeNewETable, GatherRuns, SearchOverflow, AddToOverflow, and ReplaceOldETable. Is there anything left?10-Jul-86 16:52:02	ET	Made ReplacePage becomes ReplacePage and ReplacePageInternal.11-Jul-86 15:27:07      RSV     Made Hash public.17-Jul-86 18:43:59	ET	In DeleteETableInternal, subtracted countToSmashAway (vs adding it) in calculationg countToMove for the LongCOPY.30-Jul-86 16:11:41      RSV     Chaned raising ETable.Error[insufficientSpace] to Volume.InsufficientSpace[currentFreeSpace, volume].  Changing DoOverflowIO to increment memory page when multiple runs per block.26-Aug-86 11:43:54      RSV     Moved Operation type to ETableInternal.  Made DoBucketIO and DoOverflowIO public. 6-Oct-86 18:17:20	ET	Split CreateETable into CreateETable and CreateETableInternal for the Verifier.20-Oct-86 12:32:18      ET      Changed lvBucketInfo.primaryFileHandle and lvBucketInfo.copyFileHandle to lvBucketInfo.fileHandle. This makes whichFile parameter of GetCurrentETFileSize go away. Fixed a bug in InsertPageGroups for groupPtrCount greater than LAST[CARDINAL].21-Oct-86 11:51:30      RSV     Type (length) problems fixed in InsertPageGroups. 4-Nov-86 17:07:02      RRR/RSV Added GetPageGroupInternal procedure. 5-Nov-86 17:07:02      RRR     Added SetTemporaryInternal procedure.17-Nov-86 14:36:58      RSV     Changed for new DiskBackingStore.11-Jan-87 16:18:36      RRR     Unknown changes25-Mar-87 17:30:25	RRR	Renamed ReplaceOldETable to ReplaceOldETableInternal.  Added capability to move modified ETables from overflow back into buckets.  Added external interface to ReplaceOldETableInternal (ReplaceOldETable). 28-Jun-88 10:45:40      RSV     Changed CreateETableInternal and DoOverflowIO because ETable.PageGroupHandle changed (from COMPUTED SEQUENCE to ARRAY DESCRIPTORs).  Use Inline.LongCOPY instead of FOR loop.29-Jul-88 18:59:54      RSV     Fixes to disable file locking upon request. 1-Dec-88 11:59:51      RSV     Fix AR 13583 bug in calculation of countToShrink overflowing CARDINAL value.